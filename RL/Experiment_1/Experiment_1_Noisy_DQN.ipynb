{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12656068",
   "metadata": {},
   "source": [
    "# Noisy Deep Q-Network (DQN) for LunarLander-v2\n",
    "\n",
    "This notebook is an implementation of a Deep Q-Network to tackle the \"LunarLander-v2\" environment in OpenAI Gym. The DQN will learn to land a spacecraft safely on the lunar surface. The goal is to achieve an average score of 195 or above over the last 50 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4457d",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22f7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import renderlab\n",
    "import itertools\n",
    "from torch.nn.init import uniform_\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f750d2",
   "metadata": {},
   "source": [
    "### Define the Hyperparameters\n",
    "\n",
    "Setting up hyperparameters like learning rate, batch size, exploration rates, and defining the file path for saving the trained DQN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73cf495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Parameters\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.995\n",
    "LEARNING_RATE = 0.0005\n",
    "solved_score = 195 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80295dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a192dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'models/noisy_dqn_lunarlander_model.pth'\n",
    "\n",
    "# Create the 'models' directory if it doesn't exist\n",
    "model_directory = os.path.dirname(model_save_path)\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "    \n",
    "folder_path = 'average_scores/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e774621",
   "metadata": {},
   "source": [
    "### Noisy Linear Layer Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04fb6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "\n",
    "        self.w_mu = nn.Parameter(torch.empty((out_size, in_size)))\n",
    "        self.w_sigma = nn.Parameter(torch.empty((out_size, in_size)))\n",
    "        self.b_mu = nn.Parameter(torch.empty((out_size)))\n",
    "        self.b_sigma = nn.Parameter(torch.empty((out_size)))\n",
    "\n",
    "        uniform_(self.w_mu, -math.sqrt(3 / in_size), math.sqrt(3 / in_size))\n",
    "        uniform_(self.b_mu, -math.sqrt(3 / in_size), math.sqrt(3 / in_size))\n",
    "        nn.init.constant_(self.w_sigma, 0.017)\n",
    "        nn.init.constant_(self.b_sigma, 0.017)\n",
    "\n",
    "    def forward(self, x, sigma=1):  # Adjust sigma as needed\n",
    "        if self.training:\n",
    "            w_noise = torch.normal(0, sigma, size=self.w_mu.size())\n",
    "            b_noise = torch.normal(0, sigma, size=self.b_mu.size())\n",
    "            return F.linear(x, self.w_mu + self.w_sigma * w_noise, self.b_mu + self.b_sigma * b_noise)\n",
    "        else:\n",
    "            return F.linear(x, self.w_mu, self.b_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c4e20",
   "metadata": {},
   "source": [
    "### Noisy DQN Model\n",
    "\n",
    "Below is the neural network that will approximate the Q-value function. The network will take the state as input and output Q-values for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e7a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = NoisyLinear(state_size, 64)\n",
    "        self.fc2 = NoisyLinear(64, 64)\n",
    "        self.fc3 = NoisyLinear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6706e",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "\n",
    "Using a replay memory to store transitions that the agent observes, allowing to reuse this data later. This helps in breaking the correlation between consecutive learning samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed327e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5491ab",
   "metadata": {},
   "source": [
    "### DQN Agent Class\n",
    "\n",
    "Encapsulating the learning mechanisms of the DQN agent, including action selection, model optimization, and interaction with the replay memory, along with managing the exploration rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c42b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.epsilon = EPS_START\n",
    "\n",
    "        self.policy_net = DQN(state_size, action_size)\n",
    "        self.target_net = DQN(state_size, action_size)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = ReplayMemory(BUFFER_SIZE)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.epsilon\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.action_size)]], dtype=torch.long)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        done_batch = torch.tensor(batch.done, dtype=torch.float32)\n",
    "\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = (next_state_values * GAMMA) * (1 - done_batch) + reward_batch\n",
    "\n",
    "        loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(EPS_END, EPS_DECAY * self.epsilon)\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            return self.policy_net(state).max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd4b58",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "Runs the LunarLander-v2 environment for a set number of episodes, gathering experiences and optimizing the agent's policy network based on these experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90422696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Average Score Last 10 Episodes: -863.8401141525865\n",
      "Episode 10, Average Score Last 10 Episodes: -201.71294184807397\n",
      "Episode 20, Average Score Last 10 Episodes: -212.97785145238953\n",
      "Episode 30, Average Score Last 10 Episodes: -144.80238784526566\n",
      "Episode 40, Average Score Last 10 Episodes: -166.91037480993901\n",
      "Episode 50, Average Score Last 10 Episodes: -169.21600050332597\n",
      "Episode 60, Average Score Last 10 Episodes: -188.67335771804545\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23300\\559712615.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mstep_count\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_steps_per_episode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23300\\1265173543.py\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23300\\2193835515.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23300\\2621324097.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, sigma)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mw_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mb_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_mu\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_sigma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw_noise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_mu\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_sigma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mb_noise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "scores = []\n",
    "average_scores = []\n",
    "max_steps_per_episode = 2000  # Maximum steps per episode to prevent very long episodes\n",
    "solved_score = 195  # Solved condition threshold\n",
    "solved = False  # Flag to indicate whether the environment is solved\n",
    "\n",
    "# Wrap the episode range with tqdm for the progress bar\n",
    "for i_episode in itertools.count():\n",
    "    state_tuple = env.reset()\n",
    "    state = state_tuple[0] if isinstance(state_tuple, tuple) else state_tuple\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step_count = 0  # Step counter for each episode\n",
    "\n",
    "    while not done and step_count < max_steps_per_episode:\n",
    "        action = agent.select_action(state)\n",
    "        output = env.step(action.item())\n",
    "        next_state = output[0]\n",
    "        reward = output[1]\n",
    "        done = output[2]\n",
    "\n",
    "        next_state = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float)\n",
    "\n",
    "        agent.memory.push(state, action, next_state, reward_tensor, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        agent.optimize_model()\n",
    "\n",
    "        step_count += 1\n",
    "\n",
    "    agent.update_target_net()\n",
    "    agent.decay_epsilon()\n",
    "    scores.append(total_reward)\n",
    "\n",
    "    # Calculate average score of the last 50 episodes\n",
    "    if len(scores) >= 50:\n",
    "        avg_score_last_50 = np.mean(scores[-50:])\n",
    "        average_scores.append(avg_score_last_50)\n",
    "\n",
    "        # Check if the environment is solved\n",
    "        if avg_score_last_50 >= solved_score and not solved:\n",
    "            print(\"*\"*125)\n",
    "            print(f\"Solved at episode: {i_episode} - Average score over the last 50 episodes: {avg_score_last_50}\")\n",
    "            torch.save(agent.policy_net.state_dict(), model_save_path)\n",
    "            print(\"*\"*125)\n",
    "            print(f\"Model successfully saved to {model_save_path}\")\n",
    "            solved = True\n",
    "            break  # Stop training since the environment is considered solved\n",
    "\n",
    "    # Output progress every 10 episodes\n",
    "    if i_episode % 10 == 0:\n",
    "        # Calculate the average of the last 10 episodes\n",
    "        avg_score_last_10 = np.mean(scores[-10:])\n",
    "        print(f\"Episode {i_episode}, Average Score Last 10 Episodes: {avg_score_last_10}\")\n",
    "    \n",
    "    # Output progress for every episode\n",
    "    #print(f\"Episode {i_episode}, Score: {total_reward}\")\n",
    "    \n",
    "# Save the file\n",
    "np.save(folder_path + 'average_scores_noisy.npy', average_scores)  \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5394c37",
   "metadata": {},
   "source": [
    "### Training Progress Visualization\n",
    "\n",
    "Visualizing the training progress with a plot of the average reward per episode (Y-axis) against the number of episodes (X-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ee6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the plot\n",
    "episode_numbers = list(range(10, len(average_scores) * 10 + 1, 10))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(episode_numbers, average_scores, label='Average Reward Noisy DQN', color='orange')\n",
    "plt.axhline(y=solved_score, color='green', linestyle='--', label='Solved Threshold (195)')\n",
    "\n",
    "# Set x-axis labels to show every 100 episodes\n",
    "plt.xticks(range(0, len(average_scores) + 1, 100))\n",
    "\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward per Episode in LunarLander-v2 (Noisy DQN)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad92de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52828fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the environment\n",
    "#env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "#\n",
    "## Wrap the environment with RenderFrame from renderlab to record video\n",
    "#env = renderlab.RenderFrame(env, \"./video\")\n",
    "#\n",
    "## Initialize your DQN agent here\n",
    "#state_size = env.observation_space.shape[0]\n",
    "#action_size = env.action_space.n\n",
    "#agent = DQNAgent(state_size, action_size)\n",
    "#agent.policy_net.load_state_dict(torch.load(model_save_path))\n",
    "#\n",
    "## Reset the environment and get the initial observation\n",
    "#obs, info = env.reset()\n",
    "#\n",
    "#while True:\n",
    "#    # Convert the observation to a tensor and pass it to the agent to select an action\n",
    "#    obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "#    action = agent.select_action(obs_tensor).item()\n",
    "#\n",
    "#    # Take the action in the environment\n",
    "#    obs, reward, terminated, truncated, info = env.step(action)\n",
    "#\n",
    "#    # Check if the episode is done\n",
    "#    if terminated or truncated:\n",
    "#        break\n",
    "#\n",
    "## Play the recorded video\n",
    "#env.play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
