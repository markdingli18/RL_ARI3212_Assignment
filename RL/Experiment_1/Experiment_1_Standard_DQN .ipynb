{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12656068",
   "metadata": {},
   "source": [
    "# Standard Deep Q-Network (DQN) for LunarLander-v2\n",
    "\n",
    "This notebook is an implementation of a Standard Deep Q-Network to tackle the \"LunarLander-v2\" environment in OpenAI Gym. The Standard DQN will learn to land a spacecraft safely on the lunar surface. The goal is to achieve an average score of 195 or above over the last 50 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4457d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22f7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import renderlab\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859caf19",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "Initializing the LunarLander-v2 environment and determining the state and action sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20d160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Lunar Lander environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "# Get the size of the state space\n",
    "state_size = env.observation_space.shape[0]  \n",
    "# Get the number of actions available\n",
    "action_size = env.action_space.n  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f750d2",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73cf495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Parameters\n",
    "\n",
    "# Discount factor for future rewards\n",
    "GAMMA = 0.99  \n",
    "# Size of the batch used in learning\n",
    "BATCH_SIZE = 128 \n",
    "# Maximum size of the replay buffer\n",
    "BUFFER_SIZE = 10000  \n",
    "# Starting value of epsilon for the epsilon-greedy policy\n",
    "EPS_START = 1.0 \n",
    "# Minimum value of epsilon for the epsilon-greedy policy\n",
    "EPS_END = 0.01\n",
    "# Decay rate of epsilon per episode\n",
    "EPS_DECAY = 0.995  \n",
    "# Learning rate for the neural network optimizer\n",
    "LEARNING_RATE = 0.0005  \n",
    "\n",
    "# Define the Variables\n",
    "\n",
    "# Threshold score for considering the environment solved\n",
    "solved_score = 195\n",
    "# List to keep track of total reward per episode\n",
    "scores = []  \n",
    "# List to keep track of average score over the last 50 episodes\n",
    "average_scores = []  \n",
    "# Maximum steps per episode to prevent very long episodes\n",
    "max_steps_per_episode = 2000 \n",
    "# Flag to indicate whether the environment is solved\n",
    "solved = False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c276746f",
   "metadata": {},
   "source": [
    "## Setting Up Model Saving Paths\n",
    "\n",
    "Defining and preparing the file paths for saving the trained model and average score data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7584dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where the trained model will be saved\n",
    "model_save_path = 'models/standard_dqn_lunarlander_model.pth'\n",
    "\n",
    "# Extract the directory path from the model save path\n",
    "model_directory = os.path.dirname(model_save_path)\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(model_directory):\n",
    "    # If the directory does not exist, create it\n",
    "    os.makedirs(model_directory)\n",
    "    \n",
    "# Define the path for saving average score data\n",
    "folder_path = 'average_scores/'\n",
    "\n",
    "# Check if the directory for average scores exists\n",
    "if not os.path.exists(folder_path):\n",
    "    # If the directory does not exist, create it\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c4e20",
   "metadata": {},
   "source": [
    "## Defining the Standard DQN Model\n",
    "\n",
    "Defining the neural network architecture for the Standard DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e7a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STANDARD_DQN(nn.Module):\n",
    "    # The constructor of the STANDARD_DQN class, inheriting from nn.Module\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Initialize the superclass\n",
    "        super(STANDARD_DQN, self).__init__()\n",
    "\n",
    "        # Define the first fully connected layer\n",
    "        # It takes the state size as input and outputs 64 features\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "\n",
    "        # Define the second fully connected layer\n",
    "        # It takes the 64 features from the previous layer and outputs 64 features\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "\n",
    "        # Define the third fully connected layer\n",
    "        # It takes the 64 features from the previous layer and outputs the number of actions\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    # The forward pass of the network\n",
    "    def forward(self, x):\n",
    "        # Apply a ReLU activation function after the first fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Apply a ReLU activation function after the second fully connected layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # The final layer outputs the Q-values for each action\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6706e",
   "metadata": {},
   "source": [
    "## Implementing Replay Memory\n",
    "\n",
    "Implementing a replay buffer to store and sample experiences for training the Standared DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed327e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a namedtuple to store the experience tuples\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    # Constructor for ReplayMemory class\n",
    "    def __init__(self, capacity):\n",
    "        # Initialize a double-ended queue with a fixed maximum size (capacity)\n",
    "        # This deque will store the transitions\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    # Method to push a new transition into the memory\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    # Method to randomly sample a batch of transitions from the memory\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    # Method to return the length of the memory\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5491ab",
   "metadata": {},
   "source": [
    "## Creating the Standard DQN Agent\n",
    "\n",
    "Creating a class for the Standard DQN agent encapsulating action selection, model updates, and interaction with replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c42b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standard_DQNAgent:\n",
    "    # Constructor for the standard DQN Agent\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Number of features in the state space\n",
    "        self.state_size = state_size  \n",
    "        # Number of actions available to the agent\n",
    "        self.action_size = action_size  \n",
    "        # Initialize epsilon for the epsilon-greedy strategy\n",
    "        self.epsilon = EPS_START  \n",
    "\n",
    "        # Create two neural networks: policy_net for learning and target_net for target values\n",
    "        self.policy_net = STANDARD_DQN(state_size, action_size)\n",
    "        self.target_net = STANDARD_DQN(state_size, action_size)\n",
    "        # Initialize target_net with the weights of policy_net\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        # Set target_net in evaluation mode\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # Optimizer for training the policy_net\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        # Replay memory to store experiences\n",
    "        self.memory = ReplayMemory(BUFFER_SIZE)\n",
    "\n",
    "    # Method to select an action based on the current state\n",
    "    def select_action(self, state):\n",
    "        # Random sample for epsilon-greedy strategy\n",
    "        sample = random.random()  \n",
    "        # Current value of epsilon\n",
    "        eps_threshold = self.epsilon  \n",
    "        # Epsilon-greedy policy: choose best action or random action\n",
    "        if sample > eps_threshold:\n",
    "            # Disable gradient calculation\n",
    "            with torch.no_grad():  \n",
    "                # Choose the action with the highest Q-value for the current state\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            # Choose a random action\n",
    "            return torch.tensor([[random.randrange(self.action_size)]], dtype=torch.long)\n",
    "\n",
    "    # Method to optimize the model (update the policy network)\n",
    "    def optimize_model(self):\n",
    "        # If not enough samples, return without training\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # Sample a batch of transitions from memory\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Create a mask of non-final states\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "        # Concatenate non-final next states\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        # Concatenate batches of states, actions, rewards, and done flags\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        done_batch = torch.tensor(batch.done, dtype=torch.float32)\n",
    "\n",
    "        # Calculate the current Q values\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        # Initialize next state values to zero\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "        # Update next state values using target_net for non-final states\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        # Calculate expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) * (1 - done_batch) + reward_batch\n",
    "\n",
    "        # Calculate loss using Mean Squared Error between current and expected Q values\n",
    "        loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        # Backpropagate the loss to update network weights\n",
    "        self.optimizer.zero_grad()  # Reset gradients to zero\n",
    "        loss.backward()  # Compute gradient of loss\n",
    "        self.optimizer.step()  # Update network weights\n",
    "\n",
    "    # Method to update the weights of the target_net with policy_net\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    # Method to decay the epsilon value\n",
    "    def decay_epsilon(self):\n",
    "        # Update epsilon value using the decay rate, ensuring it's not below the minimum\n",
    "        self.epsilon = max(EPS_END, EPS_DECAY * self.epsilon)\n",
    "        \n",
    "# Instantiate the Standard DQN agent\n",
    "agent = Standard_DQNAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd4b58",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "Running the training process for the agent, including interacting with the environment, optimizing the model, and tracking progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90422696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Average Score Last 10 Episodes: -167.31703230740982\n",
      "Episode 10, Average Score Last 10 Episodes: -170.5111569817062\n",
      "Episode 20, Average Score Last 10 Episodes: -135.2974810033771\n",
      "Episode 30, Average Score Last 10 Episodes: -137.36880729180956\n",
      "Episode 40, Average Score Last 10 Episodes: -90.44723156383562\n",
      "Episode 50, Average Score Last 10 Episodes: -75.00895374899817\n",
      "Episode 60, Average Score Last 10 Episodes: -100.28081426015044\n",
      "Episode 70, Average Score Last 10 Episodes: -111.00154804646347\n",
      "Episode 80, Average Score Last 10 Episodes: -61.14697709299745\n",
      "Episode 90, Average Score Last 10 Episodes: -89.2986059032846\n",
      "Episode 100, Average Score Last 10 Episodes: -77.14025716321679\n",
      "Episode 110, Average Score Last 10 Episodes: -53.34482903605341\n",
      "Episode 120, Average Score Last 10 Episodes: -33.688117594206936\n",
      "Episode 130, Average Score Last 10 Episodes: -118.9352152658962\n",
      "Episode 140, Average Score Last 10 Episodes: -132.5838930107396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15584\\4023134206.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# Optimize the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mstep_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15584\\2185337512.py\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mnext_state_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# Update next state values using target_net for non-final states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mnext_state_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnon_final_mask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnon_final_next_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Calculate expected Q values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mexpected_state_action_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnext_state_values\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdone_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop (Iterate over episodes indefinitely or until solved)\n",
    "for i_episode in itertools.count(): \n",
    "    # Reset environment and get initial state\n",
    "    state_tuple = env.reset()  \n",
    "    state = state_tuple[0] if isinstance(state_tuple, tuple) else state_tuple\n",
    "    # Convert to torch tensor\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)  \n",
    "    # Initialize total reward for this episode\n",
    "    total_reward = 0  \n",
    "    # Flag to track if the episode has ended\n",
    "    done = False  \n",
    "    # Count the number of steps in the episode\n",
    "    step_count = 0  \n",
    "\n",
    "    while not done and step_count < max_steps_per_episode:\n",
    "        # Select an action\n",
    "        action = agent.select_action(state)  \n",
    "        # Apply action to environment\n",
    "        output = env.step(action.item())  \n",
    "        # Unpack results\n",
    "        next_state, reward, done = output[0], output[1], output[2]  \n",
    "\n",
    "        next_state = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float)\n",
    "\n",
    "        # Store experience in replay memory\n",
    "        agent.memory.push(state, action, next_state, reward_tensor, done)\n",
    "        # Update state\n",
    "        state = next_state \n",
    "        # Accumulate reward\n",
    "        total_reward += reward  \n",
    "\n",
    "        # Optimize the model\n",
    "        agent.optimize_model()\n",
    "        step_count += 1\n",
    "\n",
    "    # Update target network and decay epsilon\n",
    "    agent.update_target_net()\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "    # Store the total reward\n",
    "    scores.append(total_reward)\n",
    "\n",
    "    # Calculate and store the average score of the last 50 episodes\n",
    "    if len(scores) >= 50:\n",
    "        # Compute the average score of the most recent 50 episodes\n",
    "        avg_score_last_50 = np.mean(scores[-50:])\n",
    "        # Add this average to the list\n",
    "        average_scores.append(avg_score_last_50) \n",
    "\n",
    "        # Check if the environment is considered solved\n",
    "        # The environment is solved if the average score is >= 195 for the last 50 episodes\n",
    "        if avg_score_last_50 >= solved_score and not solved:\n",
    "            print(\"*\"*125)\n",
    "            print(f\"Solved at episode: {i_episode} - Average score over the last 50 episodes: {avg_score_last_50}\")\n",
    "            # Save the current state of the policy network for future use or evaluation\n",
    "            torch.save(agent.policy_net.state_dict(), model_save_path)\n",
    "            print(\"*\"*125)\n",
    "            # Update the flag to indicate the task is solved\n",
    "            solved = True  \n",
    "            # Exit from the training loop as the goal is achieved\n",
    "            break  \n",
    "\n",
    "    # Output progress every 10 episodes\n",
    "    if i_episode % 10 == 0:\n",
    "        # Calculate the average score of the last 10 episodes for more frequent updates\n",
    "        # If fewer than 10 scores are available, calculate the average of all available scores\n",
    "        avg_score_last_10 = np.mean(scores[-10:]) if len(scores) >= 10 else np.mean(scores)\n",
    "        # Print the recent performance every 10 episodes for monitoring progress\n",
    "        print(f\"Episode {i_episode}, Average Score Last 10 Episodes: {avg_score_last_10}\")\n",
    "\n",
    "# Save the average scores for analysis\n",
    "np.save(folder_path + 'average_scores_standard.npy', average_scores)\n",
    "\n",
    "# Close the environment\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5394c37",
   "metadata": {},
   "source": [
    "## Training Progress Visualization\n",
    "\n",
    "Visualizing the training progress with a plot of the average reward per episode (Y-axis) against the number of episodes (X-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ee6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the plot\n",
    "\n",
    "# Generate a range of episode numbers for the x-axis\n",
    "# Start from episode 10 and increment by 10 until the number of episodes recorded\n",
    "episode_numbers = list(range(10, len(average_scores) * 10 + 1, 10))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "           \n",
    "# Plot average rewards per episode\n",
    "plt.plot(episode_numbers, average_scores, label='Average Reward Standard DQN', color='blue')\n",
    "           \n",
    "# Add a horizontal line representing the solved threshold\n",
    "plt.axhline(y=solved_score, color='green', linestyle='--', label='Solved Threshold (195)')\n",
    "\n",
    "# Set x-axis labels to show every 100 episodes\n",
    "plt.xticks(range(0, len(average_scores) + 1, 100))\n",
    "\n",
    "# Set labels for x and y axes\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "           \n",
    "# Display the plot\n",
    "plt.title('Average Reward per Episode in LunarLander-v2 (Standard DQN)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
