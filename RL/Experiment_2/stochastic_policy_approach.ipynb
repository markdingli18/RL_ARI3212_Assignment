{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bdec844",
   "metadata": {},
   "source": [
    "# PPO-GAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baedcfc3",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a31513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b918e87",
   "metadata": {},
   "source": [
    "### Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c4740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment name for the Gym environment\n",
    "env_name = 'LunarLanderContinuous-v2'\n",
    "\n",
    "# Set the number of parallel environments to be used\n",
    "nenvs = 8  \n",
    "\n",
    "################# nenvs = 6  \n",
    "\n",
    "# Create the Gym environment with the specified number of parallel environments\n",
    "env = gym.vector.make(env_name, num_envs=nenvs, asynchronous=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addda663",
   "metadata": {},
   "source": [
    "### Set Hyperparameters & Other Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2062a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Hyperparameters\n",
    "\n",
    "# Discount factor for future rewards \n",
    "gamma = 0.99\n",
    "# Lambda parameter for GAE\n",
    "lmbda = 0.95\n",
    "# Epsilon parameter\n",
    "epsilon = 0.2\n",
    "# Size of the batch for training the neural network\n",
    "batchsize = 64\n",
    "# Number of times to repeat the training process per epoch\n",
    "epoch_repeat = 10\n",
    "\n",
    "# Define the Variables\n",
    "\n",
    "# Flag to indicate whether the problem is solved\n",
    "solved = False\n",
    "# The reward threshold at which the environment is considered solved\n",
    "solved_reward = 195\n",
    "# List to store rewards of the last 10 epochs\n",
    "last_10_epoch_rewards = []\n",
    "# Deque to store the rewards of the most recent 50 episodes (for tracking performance)\n",
    "results = deque(maxlen=50)\n",
    "# List to store average rewards per epoch (for analysis and visualization)\n",
    "average_rewards = []\n",
    "# Array to accumulate total rewards for each environment instance\n",
    "totreward = np.zeros(nenvs)\n",
    "# Counter for the number of steps taken in the environment\n",
    "stepcount = 0\n",
    "# Counter for the number of training epochs completed\n",
    "epoc = 0\n",
    "# Reset the environment and get the initial observation/state\n",
    "obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b649c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a87d24",
   "metadata": {},
   "source": [
    "### Define Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd09dab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor Network Definition\n",
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units=64, output_size=2):\n",
    "        super(ActorNet, self).__init__()\n",
    "        # Neural network layers\n",
    "        self.model = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(input_size, hidden_units), \n",
    "             # Tanh activation function\n",
    "            nn.Tanh(),     \n",
    "            # Second linear layer\n",
    "            nn.Linear(hidden_units, int(hidden_units/2)), \n",
    "            # Tanh activation function\n",
    "            nn.Tanh()                            \n",
    "        )\n",
    "        # Output layer for mean of action distribution\n",
    "        self.mu_head = nn.Linear(int(hidden_units/2), output_size)\n",
    "        # Output layer for standard deviation of action distribution\n",
    "        self.logstd_head = nn.Linear(int(hidden_units/2), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        # Get mean of action distribution\n",
    "        loc = torch.tanh(self.mu_head(x)) * 2  \n",
    "        # Get standard deviation of action distribution\n",
    "        scale = torch.exp(self.logstd_head(x))  \n",
    "        return loc, scale\n",
    "\n",
    "# Critic Network Definition\n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units=64, output_size=2):\n",
    "        super(CriticNet, self).__init__()\n",
    "        # Neural network layers\n",
    "        self.model = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(input_size, hidden_units),  \n",
    "            # Tanh activation function\n",
    "            nn.Tanh(),      \n",
    "            # Second linear layer\n",
    "            nn.Linear(hidden_units, int(hidden_units/2)),  \n",
    "            # Tanh activation function\n",
    "            nn.Tanh()                             \n",
    "        )\n",
    "        # Output layer for value function\n",
    "        self.value_head = nn.Linear(int(hidden_units/2), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        # Get value function output\n",
    "        value = self.value_head(x)  \n",
    "        return value\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Call forward method\n",
    "        out = self.forward(x)  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28f1c9",
   "metadata": {},
   "source": [
    "### Initialize Networks & Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964bd390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the observation and action space dimensions from the environment\n",
    "\n",
    "# Observation space dimension\n",
    "obs_dim = env.single_observation_space.shape[0]  \n",
    "# Action space dimension\n",
    "n_acts = env.single_action_space.shape[0]     \n",
    "# Size of hidden layers in neural networks\n",
    "hidden_sizes = 64 \n",
    "\n",
    "# Instantiate the Actor network\n",
    "actor_net = ActorNet(obs_dim, hidden_sizes, n_acts)\n",
    "# Instantiate the Critic network with Output size 1\n",
    "critic_net = CriticNet(obs_dim, hidden_sizes, 1) \n",
    "\n",
    "# Initialize Optimizers\n",
    "\n",
    "# Optimizer for the Actor network parameters\n",
    "actor_optimizer = optim.Adam(actor_net.parameters(), lr=0.0003)\n",
    "# Optimizer for the Critic network parameters\n",
    "critic_optimizer = optim.Adam(critic_net.parameters(), lr=0.0003)\n",
    "\n",
    "# Tensor Conversion Functions\n",
    "\n",
    "# Function to convert data to a PyTorch tensor (float type)\n",
    "T = lambda x: torch.as_tensor(x, dtype=torch.float32)\n",
    "# Function to convert data to a PyTorch tensor (integer type)\n",
    "Ti = lambda x: torch.as_tensor(x, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da7cc21",
   "metadata": {},
   "source": [
    "### Running Memory Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e756576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMem():\n",
    "    def __init__(self):\n",
    "        # Initialize the memory by calling the reset method\n",
    "        self.reset()\n",
    "\n",
    "    def store(self, obs, action, logprob, reward, done, obs_, values, values_):\n",
    "        # Store the experience in the memory\n",
    "        # Store observations\n",
    "        self.obs.append(obs)    \n",
    "        # Store actions\n",
    "        self.actions.append(action.unsqueeze(-1))  \n",
    "        # Store log probabilities of actions\n",
    "        self.logprobs.append(logprob)    \n",
    "        # Store rewards\n",
    "        self.rewards.append(reward.unsqueeze(-1))  \n",
    "        # Store done flags (episode termination flags)\n",
    "        self.dones.append(done.unsqueeze(-1))  \n",
    "        # Store next observations\n",
    "        self.obs_.append(obs_)     \n",
    "        # Store values (from critic)\n",
    "        self.values.append(values)   \n",
    "        # Store next values (from critic)\n",
    "        self.values_.append(values_)              \n",
    "\n",
    "    def batches(self, batchsize):\n",
    "        # Create batches of experiences for training\n",
    "        # Total number of experiences to sample\n",
    "        size = nenvs * memsteps                   \n",
    "        idx = list(range(size))\n",
    "        # Shuffle indices for random sampling\n",
    "        random.shuffle(idx)                       \n",
    "\n",
    "        # Stack experiences into tensors\n",
    "        b_obs = torch.stack(self.obs)\n",
    "        b_actions = torch.stack(self.actions)\n",
    "        b_logprobs = torch.stack(self.logprobs)\n",
    "        b_rewards = torch.stack(self.rewards)\n",
    "        b_dones = torch.stack(self.dones)\n",
    "        b_obs_ = torch.stack(self.obs_)\n",
    "        b_values = torch.stack(self.values)\n",
    "        b_values_ = torch.stack(self.values_)\n",
    "\n",
    "        # Calculate Generalized Advantage Estimation (GAE)\n",
    "        gaes = []\n",
    "        # Initialize GAE\n",
    "        gae = T(np.zeros(nenvs)).view(nenvs, -1)  \n",
    "        for i in range(len(b_obs) - 1, -1, -1):\n",
    "            delta = b_rewards[i] + gamma * b_values_[i] * (1 - b_dones[i]) - b_values[i]\n",
    "            gae = delta + gamma * lmbda * (1 - b_dones[i]) * gae\n",
    "            gaes.insert(0, gae)\n",
    "\n",
    "        # Reshape tensors for batching\n",
    "        b_obs = b_obs.view(size, -1)\n",
    "        b_actions = b_actions.view(size, -1)\n",
    "        b_logprobs = b_logprobs.view(size, -1)\n",
    "        b_rewards = b_rewards.view(size, -1)\n",
    "        b_dones = b_dones.view(size, -1)\n",
    "        b_obs_ = b_obs_.view(size, -1)\n",
    "        b_values = b_values.view(size, -1)\n",
    "        b_values_ = b_values_.view(size, -1)\n",
    "        b_gae = torch.stack(gaes).view(size, -1)\n",
    "\n",
    "        # Yield mini-batches of experiences\n",
    "        for batchn in range(0, len(idx), batchsize):\n",
    "            batchidx = idx[batchn:batchn+batchsize]\n",
    "            batchidx = Ti(batchidx)\n",
    "            mb_obs = torch.index_select(b_obs, 0, batchidx)\n",
    "            mb_actions = torch.index_select(b_actions, 0, batchidx)\n",
    "            mb_logprobs = torch.index_select(b_logprobs, 0, batchidx)\n",
    "            mb_rewards = torch.index_select(b_rewards, 0, batchidx)\n",
    "            mb_dones = torch.index_select(b_dones, 0, batchidx)\n",
    "            mb_obs_ = torch.index_select(b_obs_, 0, batchidx)\n",
    "            mb_values = torch.index_select(b_values, 0, batchidx)\n",
    "            mb_values_ = torch.index_select(b_values_, 0, batchidx)\n",
    "            mb_gae = torch.index_select(b_gae, 0, batchidx)\n",
    "            yield mb_obs, mb_actions, mb_logprobs, mb_rewards, mb_dones, mb_obs_, mb_values, mb_values_, mb_gae\n",
    "            \n",
    "    def reset(self):\n",
    "        # Reset the memory\n",
    "        # List to store observations\n",
    "        self.obs = []   \n",
    "        # List to store actions\n",
    "        self.actions = []  \n",
    "        # List to store log probabilities of actions\n",
    "        self.logprobs = [] \n",
    "        # List to store rewards\n",
    "        self.rewards = [] \n",
    "        # List to store done flags\n",
    "        self.dones = []    \n",
    "        # List to store next observations\n",
    "        self.obs_ = []    \n",
    "        # List to store values\n",
    "        self.values = []   \n",
    "        # List to store next values\n",
    "        self.values_ = []  \n",
    "        # List to store Generalized Advantage Estimations\n",
    "        self.gae = []      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a652049",
   "metadata": {},
   "source": [
    "### Training Function and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204d235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sim_action(policy, obs):\n",
    "    # Get the action distribution parameters (mean and standard deviation) from the policy network\n",
    "    loc, std = policy(T(obs))\n",
    "    \n",
    "    # Create a normal distribution for action selection\n",
    "    dist = Normal(loc=loc, scale=std + 1e-6)\n",
    "    \n",
    "    # Sample an action from the distribution\n",
    "    action = dist.sample()\n",
    "    \n",
    "    # Compute the log probability of the selected action\n",
    "    action_log_prob = torch.sum(dist.log_prob(action), dim=-1, keepdim=True)\n",
    "    \n",
    "    return action, action_log_prob\n",
    "\n",
    "def train(mem, gamma=0.99, batchsize=10, epoch_repeat=20, epsilon=0.2, lmbda=0.95):\n",
    "    # Train the model for a specified number of epochs\n",
    "    for epochrep in range(epoch_repeat):\n",
    "        # Iterate over batches of experiences\n",
    "        for batch in mem.batches(batchsize=batchsize):\n",
    "            # Unpack the batch\n",
    "            obs, actions, logprobs, rewards, dones, obs_, values, values_, gae = batch\n",
    "            \n",
    "            # Normalize Generalized Advantage Estimation (GAE)\n",
    "            gae = (gae - torch.mean(gae)) / (torch.std(gae) + 1e-6)\n",
    "            \n",
    "            # Calculate the target for the value function\n",
    "            target = gae + values\n",
    "            \n",
    "            # Compute the value of the current states\n",
    "            state_values = critic_net(obs)\n",
    "            \n",
    "            # Calculate the critic loss\n",
    "            critic_loss = F.smooth_l1_loss(state_values, target).mean()\n",
    "\n",
    "            # Get the new action distribution parameters from the actor network\n",
    "            new_loc, new_scale = actor_net(obs)\n",
    "            dist = Normal(loc=new_loc, scale=new_scale + 1e-6)\n",
    "            \n",
    "            # Compute the new log probabilities of the actions\n",
    "            new_logprobs = torch.sum(dist.log_prob(actions), dim=-1, keepdim=True)\n",
    "            \n",
    "            # Calculate the ratio (rho) of new and old probabilities\n",
    "            rho = torch.exp(new_logprobs - logprobs)\n",
    "            \n",
    "            # Compute the surrogate losses\n",
    "            surrgt1 = rho * gae\n",
    "            surrgt2 = rho.clamp(1 - epsilon, 1 + epsilon) * gae\n",
    "            \n",
    "            # Calculate the policy loss\n",
    "            policy_loss = -torch.minimum(surrgt1, surrgt2).mean()\n",
    "\n",
    "            # Compute the total loss and perform backpropagation\n",
    "            loss = policy_loss + 0.5 * critic_loss\n",
    "            actor_optimizer.zero_grad()\n",
    "            critic_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "# Memory steps and initialization\n",
    "memsteps = 500\n",
    "mem = RunningMem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3432829",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81404f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Average Reward (Last 10 Epochs): -124.23783787978023\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    stepcount += 1\n",
    "    # Generate action and its log probability from the current policy\n",
    "    action, action_log_prob = sim_action(actor_net, obs)\n",
    "    \n",
    "    # Execute the action in the environment and get the next observation and other info\n",
    "    next_obs, reward, terminated, truncated, _ = env.step(action.numpy())\n",
    "    \n",
    "    # Check if the episode has ended\n",
    "    done = terminated | truncated\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get the value of the current and next state from the critic network\n",
    "        values = critic_net(T(obs))\n",
    "        values_ = critic_net(T(next_obs))\n",
    "    \n",
    "    # Store the experience in the memory\n",
    "    mem.store(T(obs), action, action_log_prob, T(reward), Ti(done), T(next_obs), values, values_)\n",
    "    \n",
    "    # Update the observation\n",
    "    obs = next_obs\n",
    "    \n",
    "    # Update the total reward\n",
    "    totreward += reward\n",
    "    \n",
    "    # Check for episodes that have finished\n",
    "    doneidx = np.where(done == True)\n",
    "    for k in doneidx[0]:\n",
    "        # Store the total reward and reset it for the next episode\n",
    "        results.append(totreward[k])\n",
    "        last_10_epoch_rewards.append(totreward[k])  # Add reward to last 10 epochs list\n",
    "        totreward[k] = 0\n",
    "\n",
    "    # Perform training every 'memsteps' steps\n",
    "    if stepcount > 1 and stepcount % memsteps == 0:\n",
    "        epoc += 1\n",
    "        # Train the networks using the stored experiences\n",
    "        train(mem, gamma=gamma, batchsize=batchsize, epoch_repeat=epoch_repeat, epsilon=epsilon, lmbda=lmbda)\n",
    "        # Reset the memory after training\n",
    "        mem.reset()\n",
    "\n",
    "        # Calculate and print the average reward every 10 epochs\n",
    "        if epoc % 10 == 0:\n",
    "            avg_last_10_epochs = np.mean(last_10_epoch_rewards[-10:])\n",
    "            print(f'Epoch: {epoc}, Average Reward (Average last 10 Epochs): {avg_last_10_epochs}')\n",
    "\n",
    "            # Ensure there are at least 50 epochs before checking if solved\n",
    "            if len(results) >= 50:\n",
    "                avg_reward = np.mean(list(results)[-50:])\n",
    "                average_rewards.append(avg_reward)\n",
    "\n",
    "                # Check if the average reward of last 50 episodes meets the solved criterion\n",
    "                if avg_reward >= solved_reward and not solved:\n",
    "                    print(\"*\" * 125)\n",
    "                    print(f'Solved! Epoch: {epoc}, Average Score Last 50 Episodes: {avg_reward}')\n",
    "                    print(\"*\" * 125)\n",
    "                    solved = True\n",
    "                    # Save the model or perform any additional actions here\n",
    "                    break\n",
    "\n",
    "# Close the environment after training\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b87284f",
   "metadata": {},
   "source": [
    "### Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea10854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the plot\n",
    "\n",
    "# Generate a range of episode numbers for the x-axis\n",
    "episode_numbers = list(range(1, len(average_rewards) + 1))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot average rewards per episode\n",
    "plt.plot(episode_numbers, average_rewards, label='Average Reward PPO-GAE', color='blue')\n",
    "\n",
    "# Add a horizontal line representing the solved threshold\n",
    "plt.axhline(y=solved_reward, color='green', linestyle='--', label='Solved Threshold (195)')\n",
    "\n",
    "# Set labels for x and y axes\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "\n",
    "# Display the plot\n",
    "plt.title('Average Reward per Episode in LunarLanderContinuous-v2 (PPO-GAE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82502b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
