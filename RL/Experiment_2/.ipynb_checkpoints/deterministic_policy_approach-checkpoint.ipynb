{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3fc1f68",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc445ac",
   "metadata": {},
   "source": [
    "### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9df5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Sequence\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "import random\n",
    "import copy\n",
    "from itertools import count\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c5e4c",
   "metadata": {},
   "source": [
    "### Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa45d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LunarLanderContinuous-v2 environment\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "# Reset the environment to get the initial observation and set the initial total reward to zero\n",
    "obs, _ = env.reset()  \n",
    "episode_reward = 0.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ccd741",
   "metadata": {},
   "source": [
    "### Set Hyperparameters & Other Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80719618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a namedtuple to store the experience tuples\n",
    "Transition = namedtuple('Transition', ('states', 'actions', 'rewards', 'dones', 'next_states'))\n",
    "\n",
    "# Define the Hyperparameters\n",
    "\n",
    "# Discount factor for future rewards\n",
    "GAMMA = 0.99  \n",
    "# Number of samples per batch\n",
    "BATCH_SIZE = 64  \n",
    "# Maximum size of the replay buffer\n",
    "BUFFER_SIZE = 10000  \n",
    "# Minimum size of the replay buffer \n",
    "MIN_REPLAY_SIZE = 5000  \n",
    "# Soft update parameter for the target network\n",
    "TAU = 0.01  \n",
    "\n",
    "# Define the Variables\n",
    "\n",
    "# Standard deviation for the noise process\n",
    "std_dev = 0.2\n",
    "# Deque to store the last 50 episode rewards\n",
    "returns = deque(maxlen=50)  \n",
    "# List to store average rewards per episode\n",
    "average_rewards = []  \n",
    "# Threshold for considering the environment as solved\n",
    "solved_reward = 195 \n",
    "\n",
    "# Function to convert data to a PyTorch tensor (float)\n",
    "T = lambda x: torch.as_tensor(x, dtype=torch.float32)  \n",
    "# Function to convert data to a PyTorch tensor (integer)\n",
    "Ti = lambda x: torch.as_tensor(x, dtype=torch.int64)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b93424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0917957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for saving average score data\n",
    "folder_path = 'average_scores_experiment_2/'\n",
    "\n",
    "# Check if the directory for average scores exists\n",
    "if not os.path.exists(folder_path):\n",
    "    # If the directory does not exist, create it\n",
    "    os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9644232f",
   "metadata": {},
   "source": [
    "### Define Ornstein-Uhlenbeck Noise for Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96747bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class for action noise, to be used in exploration strategies\n",
    "class ActionNoise(object):\n",
    "    def reset(self):\n",
    "        pass  \n",
    "\n",
    "# Implementation of Ornstein-Uhlenbeck process for adding noise to actions\n",
    "class OrnsteinUhlenbeckActionNoise(ActionNoise):\n",
    "    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x0=None):\n",
    "        # Speed of mean reversion\n",
    "        self.theta = theta\n",
    "        # Long-running mean\n",
    "        self.mu = mu   \n",
    "        # Volatility parameter\n",
    "        self.sigma = sigma  \n",
    "        # Time increment\n",
    "        self.dt = dt  \n",
    "        # Initial value for the process\n",
    "        self.x0 = x0        \n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Compute the next sample of noise\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the noise to its initial state or zero if not specified\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Representation of the noise parameters\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "\n",
    "# Create an instance of Ornstein-Uhlenbeck noise with specified parameters\n",
    "ou_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(1), sigma=float(std_dev) * np.ones(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0412a",
   "metadata": {},
   "source": [
    "### Define Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be23fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for managing the replay memory\n",
    "class Replay_memory():\n",
    "    def __init__(self, env, fullsize, minsize, batchsize):\n",
    "        # The Gym environment\n",
    "        self.env = env                 \n",
    "        # Memory buffer to store transitions with a fixed maximum size\n",
    "        self.memory = deque(maxlen=fullsize)    \n",
    "        # Batch size for sampling from the memory\n",
    "        self.batchsize = batchsize   \n",
    "        # Minimum size of memory before starting training\n",
    "        self.minsize = minsize            \n",
    "\n",
    "    def append(self, transition):\n",
    "        # Add a transition to the memory buffer\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        # Sample a batch of transitions for training\n",
    "        batch = random.sample(self.memory, self.batchsize)\n",
    "        # Convert batch to Transition namedtuple format\n",
    "        batch = Transition(*zip(*batch))  \n",
    "        # Convert arrays to PyTorch tensors\n",
    "        states = torch.from_numpy(np.array(batch.states, dtype=np.float32))\n",
    "        actions = torch.from_numpy(np.array(batch.actions, dtype=np.float32))\n",
    "        rewards = torch.from_numpy(np.array(batch.rewards, dtype=np.float32)).unsqueeze(1)\n",
    "        dones = torch.from_numpy(np.array(batch.dones, dtype=np.bool8)).unsqueeze(1)\n",
    "        next_states = torch.from_numpy(np.array(batch.next_states, dtype=np.float32))\n",
    "        return states, actions, rewards, dones, next_states\n",
    "\n",
    "    def initialize(self):\n",
    "        # Populate the memory with minimum required samples before training\n",
    "        obs, _ = env.reset()\n",
    "        for _ in range(self.minsize):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            transition = Transition(obs, action, reward, done, new_obs)\n",
    "            self.append(transition)\n",
    "            obs = new_obs\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "        return self\n",
    "\n",
    "# Initialize the replay memory with the specified environment and parameters\n",
    "replay_memory = Replay_memory(env, BUFFER_SIZE, MIN_REPLAY_SIZE, BATCH_SIZE).initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a69365",
   "metadata": {},
   "source": [
    "### Define Actor (Policy) and Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d17445f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PolicyNet: Defines the actor network for generating actions based on states\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units, output_size, pmin, pmax):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        # Minimum and maximum action values (used for clipping the output)\n",
    "        self.pmin = pmin\n",
    "        self.pmax = pmax\n",
    "        # Define the neural network layers\n",
    "        self.model = nn.Sequential(\n",
    "            # Input layer \n",
    "            nn.Linear(input_size, hidden_units),  \n",
    "            # Activation function\n",
    "            nn.ReLU(),             \n",
    "            # Hidden layer with half the units of the previous layer\n",
    "            nn.Linear(hidden_units, int(hidden_units/2)),  \n",
    "            # Activation function\n",
    "            nn.ReLU(),  \n",
    "            # Output layer with 'output_size' units\n",
    "            nn.Linear(int(hidden_units/2), output_size),  \n",
    "            # Activation function to scale the output\n",
    "            nn.Tanh()                             \n",
    "        )\n",
    "    \n",
    "    # Forward pass through the network\n",
    "    def forward(self, x):\n",
    "        # Scale the output by the maximum action value\n",
    "        x = self.model(x) * self.pmax  \n",
    "        # Clip the output to be within the specified range\n",
    "        torch.clip_(x, self.pmin, self.pmax)  \n",
    "        return x\n",
    "    \n",
    "# DQN (CriticNet): Defines the critic network for evaluating state-action pairs\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units):\n",
    "        super(DQN, self).__init__()\n",
    "        # Define the neural network layers\n",
    "        self.model = nn.Sequential(\n",
    "            # Input layer\n",
    "            nn.Linear(input_size, hidden_units), \n",
    "            # Activation function\n",
    "            nn.ReLU(),       \n",
    "            # Hidden layer\n",
    "            nn.Linear(hidden_units, int(hidden_units/2)),  \n",
    "            # Activation function\n",
    "            nn.ReLU(),                         \n",
    "            # Output layer with a single unit\n",
    "            nn.Linear(int(hidden_units/2), 1)     \n",
    "        )\n",
    "    \n",
    "    # Forward pass for computing Q-values\n",
    "    def forward(self, state, action):\n",
    "        # Concatenate state and action as input\n",
    "        x = torch.cat([state, action], 1)  \n",
    "        # Pass the concatenated input through the network\n",
    "        x = self.model(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb43f151",
   "metadata": {},
   "source": [
    "### Initialize Networks and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "092857a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the size of the observation and action spaces from the environment\n",
    "obs_size = env.observation_space.shape[0]  \n",
    "act_size = env.action_space.shape[0]       \n",
    "\n",
    "# Define the number of hidden layers for the neural networks\n",
    "hiddenlayers = 128\n",
    "\n",
    "# Get the minimum and maximum range for actions from the environment\n",
    "output_minrange = env.action_space.low     \n",
    "output_maxrange = env.action_space.high    \n",
    "\n",
    "# Initialize the actor network (PolicyNet) with the environment's dimensions and action range\n",
    "actor = PolicyNet(obs_size, hiddenlayers, act_size, T(output_minrange), T(output_maxrange))\n",
    "\n",
    "# Create a target actor network as a deep copy for stable training\n",
    "actor_target = copy.deepcopy(actor)\n",
    "\n",
    "# Initialize the critic network (DQN) with combined dimensions of state and action space\n",
    "critic = DQN(obs_size + act_size, hiddenlayers)\n",
    "\n",
    "# Create a target critic network as a deep copy for stable training\n",
    "critic_target = copy.deepcopy(critic)\n",
    "\n",
    "# Initialize optimizers for both actor and critic networks with specified learning rates\n",
    "actor_optimizer = optim.AdamW(actor.parameters(), lr=0.0003)\n",
    "critic_optimizer = optim.AdamW(critic.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc11e4",
   "metadata": {},
   "source": [
    "### Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63c76ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    # Sample a batch of experiences from the replay memory\n",
    "    states, actions, rewards, dones, next_states = replay_memory.sample_batch()\n",
    "\n",
    "    # Reset the Ornstein-Uhlenbeck noise for exploration\n",
    "    ou_noise.reset()\n",
    "\n",
    "    # Calculate the critic loss\n",
    "    Qvals = critic(states, actions)  # Predict the current Q-values\n",
    "    with torch.no_grad():\n",
    "        # Predict next actions using the target actor network\n",
    "        actions_ = actor_target(next_states)      \n",
    "        # Predict next Q-values using the target critic network\n",
    "        Qvals_ = critic_target(next_states, actions_)   \n",
    "        # Set Q-values to 0 for terminal states\n",
    "        Qvals_[dones] = 0.0                  \n",
    "        # Compute the target Q-values\n",
    "        target = rewards + GAMMA * Qvals_      \n",
    "    # Compute the loss between target and current Q-values\n",
    "    critic_loss = F.smooth_l1_loss(target, Qvals)       \n",
    "\n",
    "    # Calculate the actor loss (negative mean of the critic's Q-values)\n",
    "    actor_loss = -critic(states, actor(states)).mean()\n",
    "\n",
    "    # Update the actor network\n",
    "    # Zero the gradients\n",
    "    actor_optimizer.zero_grad()  \n",
    "    # Backpropagate the loss\n",
    "    actor_loss.backward()  \n",
    "    # Perform a step of optimization\n",
    "    actor_optimizer.step()       \n",
    "\n",
    "    # Update the critic network\n",
    "    # Zero the gradients\n",
    "    critic_optimizer.zero_grad()  \n",
    "    # Backpropagate the loss\n",
    "    critic_loss.backward() \n",
    "    # Perform a step of optimization\n",
    "    critic_optimizer.step()       \n",
    "\n",
    "    # Update the target networks\n",
    "    for target_param, param in zip(actor_target.parameters(), actor.parameters()):\n",
    "        # Update target actor network\n",
    "        target_param.data.copy_(param.data * TAU + target_param.data * (1.0 - TAU))  \n",
    "    for target_param, param in zip(critic_target.parameters(), critic.parameters()):\n",
    "        # Update target critic network\n",
    "        target_param.data.copy_(param.data * TAU + target_param.data * (1.0 - TAU))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874db136",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc3742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  Reward: -92.14185303519713  Average Reward (Last 10 Episodes): -92.14185303519713\n",
      "Episode: 10  Reward: -57.13933618644114  Average Reward (Last 10 Episodes): -147.596421189685\n",
      "Episode: 20  Reward: -188.72736158061565  Average Reward (Last 10 Episodes): -133.53293618743368\n",
      "Episode: 30  Reward: -47.93701079714593  Average Reward (Last 10 Episodes): -104.95063778526121\n"
     ]
    }
   ],
   "source": [
    "# Training loop for each episode\n",
    "for episode in count():\n",
    "    # Reset the environment at the start of each episode\n",
    "    state, _ = env.reset()\n",
    "    # Initialize the reward for the current episode\n",
    "    episode_reward = 0  \n",
    "    # Flag to track if the episode is finished\n",
    "    done = False       \n",
    "\n",
    "    # Loop until the episode ends\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            # Get action from the actor network plus some noise for exploration\n",
    "            action = actor(T(state)).numpy() + ou_noise()\n",
    "        # Apply the action to the environment and observe the next state and reward\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        # Check if the episode has ended\n",
    "        done = terminated or truncated  \n",
    "        # Create a transition tuple for the experience\n",
    "        transition = Transition(state, action, reward, done, new_state)\n",
    "        # Store the transition in replay memory\n",
    "        replay_memory.append(transition)\n",
    "        # Update the networks\n",
    "        update()\n",
    "        # Update the state for the next iteration\n",
    "        state = new_state\n",
    "        # Accumulate the reward\n",
    "        episode_reward += reward\n",
    "\n",
    "    # Store the total reward of the episode\n",
    "    returns.append(episode_reward)\n",
    "    average_rewards.append(episode_reward)\n",
    "    \n",
    "    # Print the reward every 10 episodes and the average of the last 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        # Calculate average of the last 10 episode rewards\n",
    "        last_10_avg_reward = np.mean(average_rewards[-10:])  \n",
    "        print(f'Episode: {episode}  Reward: {episode_reward}  Average Reward (Last 10 Episodes): {last_10_avg_reward}')\n",
    "\n",
    "    # Check if the problem is solved (average reward over last 50 episodes >= solved_reward)\n",
    "    if len(returns) == 50:\n",
    "        avg_reward = np.mean(returns)\n",
    "        if avg_reward >= solved_reward:\n",
    "            # Print a message if the environment is considered solved\n",
    "            print(\"*\" * 125)\n",
    "            print(f'Solved! Episode: {episode}, Average Score Last 50 Episodes: {avg_reward}')\n",
    "            print(\"*\" * 125)\n",
    "            # Break the loop if the environment is solved\n",
    "            break\n",
    "\n",
    "# Save average reward to file\n",
    "np.save(folder_path + 'average_scores_deterministic.npy', np.array(average_rewards))\n",
    "\n",
    "# Close the environment after training\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c4b60",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the plot for DDPG\n",
    "\n",
    "# Assuming average_rewards contains the average reward for each episode\n",
    "episode_numbers = list(range(1, len(average_rewards) + 1))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot average rewards per episode\n",
    "plt.plot(episode_numbers, average_rewards, label='Average Reward DDPG', color='red')\n",
    "\n",
    "# Add a horizontal line representing the solved threshold\n",
    "plt.axhline(y=solved_reward, color='green', linestyle='--', label='Solved Threshold (195)')\n",
    "\n",
    "# Set labels for x and y axes\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "\n",
    "# Display the plot\n",
    "plt.title('Average Reward per Episode in LunarLanderContinuous-v2 (DDPG)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
