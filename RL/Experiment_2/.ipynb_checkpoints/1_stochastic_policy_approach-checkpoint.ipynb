{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bdec844",
   "metadata": {},
   "source": [
    "# PPO-GAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baedcfc3",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a31513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b918e87",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c4740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'LunarLanderContinuous-v2'\n",
    "nenvs = 6  # Adjusted for computational efficiency\n",
    "env = gym.vector.make(env_name, num_envs=nenvs, asynchronous=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a87d24",
   "metadata": {},
   "source": [
    "### Define Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd09dab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units=64, output_size=2):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_units),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_units, int(hidden_units/2)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.mu_head = nn.Linear(int(hidden_units/2), output_size)\n",
    "        self.logstd_head = nn.Linear(int(hidden_units/2), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        loc = torch.tanh(self.mu_head(x)) * 2  # Adjust the range if needed\n",
    "        scale = torch.exp(self.logstd_head(x))\n",
    "        return loc, scale\n",
    "    \n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units=64, output_size=2):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_units),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_units, int(hidden_units/2)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.value_head = nn.Linear(int(hidden_units/2), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        value = self.value_head(x)\n",
    "        return value\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.forward(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28f1c9",
   "metadata": {},
   "source": [
    "### Initialize Networks, Optimizers, and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964bd390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Networks\n",
    "obs_dim = env.single_observation_space.shape[0]\n",
    "n_acts = env.single_action_space.shape[0]\n",
    "hidden_sizes = 64  # Can be tuned\n",
    "\n",
    "actor_net = ActorNet(obs_dim, hidden_sizes, n_acts)\n",
    "critic_net = CriticNet(obs_dim, hidden_sizes, 1)\n",
    "\n",
    "# Initialize Optimizers\n",
    "actor_optimizer = optim.Adam(actor_net.parameters(), lr=0.0003)\n",
    "critic_optimizer = optim.Adam(critic_net.parameters(), lr=0.0003)\n",
    "\n",
    "# Tensor Conversion Functions\n",
    "T = lambda x: torch.as_tensor(x, dtype=torch.float32)\n",
    "Ti = lambda x: torch.as_tensor(x, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da7cc21",
   "metadata": {},
   "source": [
    "### Training Function and Other Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e756576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMem():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def store(self, obs, action, logprob, reward, done, obs_, values, values_):\n",
    "        self.obs.append(obs)\n",
    "        self.actions.append(action.unsqueeze(-1))\n",
    "        self.logprobs.append(logprob)\n",
    "        self.rewards.append(reward.unsqueeze(-1))\n",
    "        self.dones.append(done.unsqueeze(-1))\n",
    "        self.obs_.append(obs_)\n",
    "        self.values.append(values)\n",
    "        self.values_.append(values_)\n",
    "\n",
    "\n",
    "    def batches(self, batchsize):\n",
    "        size = nenvs*memsteps\n",
    "        idx = list(range(size))\n",
    "        random.shuffle(idx)\n",
    "\n",
    "        b_obs = torch.stack(self.obs)\n",
    "        b_actions = torch.stack(self.actions)\n",
    "        b_logprobs = torch.stack(self.logprobs)\n",
    "        b_rewards = torch.stack(self.rewards)\n",
    "        b_dones = torch.stack(self.dones)\n",
    "        b_obs_ = torch.stack(self.obs_)\n",
    "        b_values = torch.stack(self.values)\n",
    "        b_values_ = torch.stack(self.values_)\n",
    "\n",
    "        gaes = []\n",
    "        gae = T(np.zeros(nenvs)).view(nenvs,-1)\n",
    "        for i in range(len(b_obs)-1,-1,-1):\n",
    "            delta = b_rewards[i] + gamma * b_values_[i] * (1-b_dones[i]) - b_values[i]\n",
    "            gae = delta + gamma * lmbda * (1-b_dones[i]) * gae\n",
    "            gaes.insert(0, gae)\n",
    "\n",
    "        b_obs = b_obs.view(size, -1)\n",
    "        b_actions = b_actions.view(size, -1)\n",
    "        b_logprobs = b_logprobs.view(size, -1)\n",
    "        b_rewards = b_rewards.view(size, -1)\n",
    "        b_dones = b_dones.view(size, -1)\n",
    "        b_obs_ = b_obs_.view(size, -1)\n",
    "        b_values = b_values.view(size, -1)\n",
    "        b_values_ = b_values_.view(size, -1)\n",
    "        b_gae = torch.stack(gaes).view(size, -1)\n",
    "\n",
    "        for batchn in range(0, len(idx), batchsize):\n",
    "            batchidx = idx[batchn:batchn+batchsize]\n",
    "            batchidx = Ti(batchidx)\n",
    "            mb_obs = torch.index_select(b_obs, 0, batchidx)\n",
    "            mb_actions = torch.index_select(b_actions, 0, batchidx)\n",
    "            mb_logprobs = torch.index_select(b_logprobs, 0, batchidx)\n",
    "            mb_rewards = torch.index_select(b_rewards, 0, batchidx)\n",
    "            mb_dones = torch.index_select(b_dones, 0, batchidx)\n",
    "            mb_obs_ = torch.index_select(b_obs_, 0, batchidx)\n",
    "            mb_values = torch.index_select(b_values, 0, batchidx)\n",
    "            mb_values_ = torch.index_select(b_values_, 0, batchidx)\n",
    "            mb_gae = torch.index_select(b_gae, 0, batchidx)\n",
    "            yield mb_obs, mb_actions, mb_logprobs, mb_rewards, mb_dones, mb_obs_, mb_values, mb_values_, mb_gae\n",
    "\n",
    "    def reset(self):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.obs_ = []\n",
    "        self.values = []\n",
    "        self.values_ = []\n",
    "        self.gae = []\n",
    "        \n",
    "@torch.no_grad()\n",
    "def sim_action(policy, obs):\n",
    "    loc, std = policy(T(obs))\n",
    "    dist = Normal(loc=loc, scale=std+1e-6)\n",
    "    action = dist.sample()\n",
    "    action_log_prob = torch.sum(dist.log_prob(action), dim=-1, keepdim=True)\n",
    "    return action, action_log_prob\n",
    "\n",
    "def train(mem, gamma=0.99, batchsize=10, epoch_repeat=20, epsilon=0.2, lmbda=0.95):\n",
    "    for epochrep in range(epoch_repeat):\n",
    "        for batch in mem.batches(batchsize=batchsize):\n",
    "            obs, actions, logprobs, rewards, dones, obs_, values, values_, gae = batch\n",
    "            gae = (gae - torch.mean(gae)) / (torch.std(gae) + 1e-6)\n",
    "            target = gae + values\n",
    "            state_values = critic_net(obs)\n",
    "            critic_loss = F.smooth_l1_loss(state_values, target).mean()\n",
    "\n",
    "            new_loc, new_scale = actor_net(obs)\n",
    "            dist = Normal(loc=new_loc, scale=new_scale+1e-6)\n",
    "            new_logprobs = torch.sum(dist.log_prob(actions), dim=-1, keepdim=True)\n",
    "            rho = torch.exp(new_logprobs - logprobs)\n",
    "            surrgt1 = rho * gae\n",
    "            surrgt2 = rho.clamp(1-epsilon, 1+epsilon) * gae\n",
    "            policy_loss = -torch.minimum(surrgt1, surrgt2).mean()\n",
    "\n",
    "            loss = policy_loss + 0.5*critic_loss\n",
    "            actor_optimizer.zero_grad()\n",
    "            critic_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3432829",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81404f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoc: 1 Avg Result: -210.9527172138875\n",
      "Epoc: 2 Avg Result: -173.79107506392208\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9708\\4180179688.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstepcount\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mstepcount\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmemsteps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mepoc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_repeat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch_repeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlmbda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mmem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9708\\2866454141.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(mem, gamma, batchsize, epoch_repeat, epsilon, lmbda)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mactor_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m             \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m                 \u001b[1;31m# call optimizer step pre hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mpre_hook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_global_optimizer_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\profiler.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_function_enter_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[1;31m# TODO: use this to make a __dir__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = deque(maxlen=50)\n",
    "average_rewards = []  # List to store average rewards per epoch\n",
    "memsteps = 500\n",
    "mem = RunningMem()\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "epsilon = 0.2\n",
    "batchsize = 64\n",
    "epoch_repeat = 10\n",
    "solved = False  # Flag to indicate whether the problem is solved\n",
    "\n",
    "totreward = np.zeros(nenvs)\n",
    "stepcount = 0\n",
    "epoc = 0\n",
    "obs, _ = env.reset()\n",
    "\n",
    "while True:\n",
    "    stepcount += 1\n",
    "    action, action_log_prob = sim_action(actor_net, obs)\n",
    "    next_obs, reward, terminated, truncated, _ = env.step(action.numpy())\n",
    "    done = terminated | truncated\n",
    "    with torch.no_grad():\n",
    "        values = critic_net(T(obs))\n",
    "        values_ = critic_net(T(next_obs))\n",
    "    mem.store(T(obs), action, action_log_prob, T(reward), Ti(done), T(next_obs), values, values_)\n",
    "    obs = next_obs\n",
    "    totreward += reward\n",
    "    doneidx = np.where(done == True)\n",
    "    for k in doneidx[0]:\n",
    "        results.append(totreward[k])\n",
    "        totreward[k] = 0\n",
    "\n",
    "    if stepcount > 1 and stepcount % memsteps == 0:\n",
    "        epoc += 1\n",
    "        train(mem, gamma=gamma, batchsize=batchsize, epoch_repeat=epoch_repeat, epsilon=epsilon, lmbda=lmbda)\n",
    "        mem.reset()\n",
    "\n",
    "        # Calculate the average reward over the last 50 episodes\n",
    "        if len(results) >= 50:\n",
    "            avg_reward = np.mean(list(results)[-50:])  # Average of last 50 elements in deque\n",
    "        else:\n",
    "            avg_reward = np.mean(results)  # Average of all elements if less than 50\n",
    "\n",
    "        average_rewards.append(avg_reward)  # Store the average reward\n",
    "        print(f'Epoc: {epoc} Avg Result: {avg_reward}')  # Print every epoch\n",
    "\n",
    "        # Check if the average of the last 50 episodes is >= 195\n",
    "        if avg_reward >= 195 and not solved:\n",
    "            print(\"*\" * 125)\n",
    "            print(f'Solved! Epoc: {epoc}, Average Score Last 50 Episodes: {avg_reward}')\n",
    "            print(\"*\" * 125)\n",
    "            solved = True  # Set the flag to indicate the task is solved\n",
    "            # Save the model or any additional actions here\n",
    "            break\n",
    "\n",
    "# Optional: Save the average_rewards for analysis\n",
    "# np.save('<path_to_save>/average_rewards.npy', np.array(average_rewards))\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b87284f",
   "metadata": {},
   "source": [
    "### Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea10854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the plot\n",
    "\n",
    "# Generate a range of episode numbers for the x-axis\n",
    "episode_numbers = list(range(1, len(average_rewards) + 1))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot average rewards per episode\n",
    "plt.plot(episode_numbers, average_rewards, label='Average Reward PPO-GAE', color='blue')\n",
    "\n",
    "# Add a horizontal line representing the solved threshold\n",
    "solved_score = 195\n",
    "plt.axhline(y=solved_score, color='green', linestyle='--', label='Solved Threshold (195)')\n",
    "\n",
    "# Set labels for x and y axes\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "\n",
    "# Display the plot\n",
    "plt.title('Average Reward per Episode in LunarLanderContinuous-v2 (PPO-GAE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
