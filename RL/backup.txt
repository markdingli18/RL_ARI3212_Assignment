\documentclass[sigplan,screen]{acmart}
\AtBeginDocument{
  \providecommand\BibTeX{{
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}
\settopmatter{printacmref=false}
\RequirePackage[backend=biber, sorting=none]{biblatex} 
\addbibresource{sample-base.bib}
\usepackage{amsmath}
\usepackage{listings}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{algorithm}
\usepackage{algpseudocode}

\setcopyright{none}
\settopmatter{printacmref=false} 
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

\begin{document}

\pagestyle{empty}

\title{\huge Solving Control Problems using Reinforcement Learning}

\author{Mark Dingli}
\affiliation{
  \institution{University of Malta}}
\email{mark.dingli.21@um.edu.mt}

\maketitle

\section{Introduction}

\subsection{What is Reinforcement Learning?}
Reinforcement learning (RL) is branch of machine learning that enables AI-driven agents with the capability to learn through trial and error by making use of feedback from their own actions. This method involves rewarding positive behaviors and penalizing negative ones. The agent engages with its environment, undertaking various actions with the primary objective of maximizing its total reward. Over time, the agent refines its behaviors based on environmental feedback, employing advanced techniques such as Q-learning. This iterative learning process allows the agent to enhance its performance and decision-making capabilities continuously \cite{ref1}.

\subsection{How does Reinforcement Learning differ from other ML approaches?}

Machine Learning (ML) includes a diverse range of approaches, each characterized by unique learning mechanisms. Within this spectrum, Reinforcement Learning (RL) emerges as a distinct subset of ML, where learning is influenced by the outcomes of actions, either positive or negative, rather than depending on a dataset. Unlike supervised learning, which utilizes labeled data to train models, RL operates without predefined labels, learning instead from direct interaction with the environment. This approach contrasts with supervised learning, where a 'teacher' provides correct outputs for training as can be seen in \cite{ref2}. Another ML technique is unsupervised learning, which uses unlabeled data to detect patterns or structures, operating without any guidance on the expected output, as shown in \cite{ref2}. In RL, the lack of direct input necessitates the development of an optimal policy or strategy to achieve specific goals.  Essentially, while supervised and unsupervised learning aim at predictions or classifications based on data, RL trains agents to make a series of decisions within an environment, with the goal of maximizing a cumulative reward.

\subsection{Differences between Value Based, Policy Based and Actor Critic models.}
The field of reinforcement learning encompasses various models, each with unique approaches to learning and decision-making. Among these, Value Based, Policy Based, and Actor Critic models stand out due to their distinct methodologies and applications. 

\subsubsection{Value Based Models}
\hfill \break 
 As outlined in \cite{ref3}, value-Based models primarily focus on learning the value or action-value function to derive the optimal policy. In these models, an agent is employed to learn a value function, which estimates the long-term reward linked with each state or state-action pair. The core objective of value-based methods is to determine the value of states, or to estimate the value of states and actions. This is accomplished through the learning of value functions or Q-functions. The agent uses these value function to select actions that maximise the expected reward.

\subsubsection{Policy Based Models}
\hfill \break 
Policy-Based models in machine learning diverge from the value function approach, focusing instead on directly learning a policy. These methods entirely avoid the process of learning the values of states or actions. The primary aim of policy-based methods is to directly determine the optimal policy. This policy defines the strategy that an agent should adopt for selecting actions. Typically, the policy is parameterized, and these parameters are iteratively adjusted to optimize the expected return. This approach allows for a more direct route to determining the best action choices, without the intermediary step of value estimation as detailed in \cite{ref4}.

\subsubsection{Actor Critic Models}
\hfill \break 
Actor-Critic based models are a combination of value-based and policy-based methodologies. In these models, an agent simultaneously learns a value and a policy function. This combination leverages the strengths of both value-based and policy-based models. The \textit{'actor'} component in these models is responsible for determining the course of action, functioning as a policy model. In contrast, the \textit{'critic'} serves as a value model, appraising the actions taken by the actor. The feedback provided by the critic is instrumental in refining the strategies of both the actor and the critic itself. The learning process for the actor is grounded in the policy gradient method, which focuses on enhancing the policy directly. Meanwhile, the critic's role is to assess the actor's actions by calculating the value function as described in \cite{ref5}. 

\hfill \break 
In summary, Value Based models prioritize learning the value of states and actions to make optimal decisions, while Policy Based models focus directly on learning the optimal policy. Actor Critic models blend these approaches, leveraging the strengths of both to create a more robust and dynamic learning process. 

\section{Background}

\subsection{Value Based Methods}
Value Based Methods in Reinforcement Learning focus on estimating the potential long-term rewards for different states or actions in a given environment. These methods differ from other RL strategies since they do not directly learn or define a policy. A policy is the process through which an agent chooses actions based on the current state. Instead, Value Based Methods focus on evaluating the 'value' of each state or state-action pair. This 'value' represents the expected total reward that the agent can obtain, beginning from that state or after executing a specific action. The goal is to employ these value estimations to steer the agent's decisions, encouraging actions that optimize these expected rewards. This approach is essential in identifying the most advantageous actions an agent can take to achieve its goals efficiently \cite{ref3}.\hfill \break 

An example of value-based methods is Q-Learning. This off-policy algorithm aims to learn the value of an action in a specific state, bypassing the need for a model of the environment. It achieves this by learning the optimal policy indirectly through the value of the best action in each state. An essential component of Q-Learning is its update equation, which iteratively refines the Q-values based on the agent's experiences.\hfill \break 

\noindent The Q-Learning Update Equation is given by:

\begin{equation*}
    Q^{new}(s, a) \leftarrow Q(s, a) + \alpha [R + \gamma \max_{a'} Q(s', a') - Q(s, a)]
\end{equation*}

In this equation, \( Q(s, a) \) represents the current estimate of the value for taking action \( a \) in state \( s \), with \( \alpha \) as the learning rate, \( R \) as the immediate reward, \( \gamma \) as the discount factor, and \( \max_{a'} Q(s', a') \) represents the estimated value of the best possible future action in the new state \( s' \).\hfill \break 

Another example is Value Iteration, used predominantly in Markov Decision Processes (MDPs). This algorithm iteratively updates the value of each state until convergence to the optimal value function is achieved. Value Iteration is especially effective in environments with a manageable state space and a known environmental model.\hfill \break 

\noindent The Bellman Equation for Value Iteration is expressed as:
\begin{equation*}
    V^*(s) = \max_a \sum_{s', r} P(s', r|s, a) [r + \gamma V^*(s')]
\end{equation*}

Here, \( V^*(s) \) is the optimal value function for state \( s \), with \( P(s', r|s, a) \) denoting the transition probability and \( \gamma \) the discount factor.\hfill \break 

In short, value-based methods offer a structured approach for agents to make informed decisions. Through algorithms like Q-Learning and Value Iteration, value-based methods focus on maximizing expected cumulative rewards over time.

\subsection{Deep Q-Networks Algorithm}
Deep Q-Networks (DQNs) are an advancement in the field of Reinforcement Learning, effectively merging the foundational principles of Q-Learning with the sophisticated capabilities of deep neural networks. This integration allows DQNs to excel in high-dimensional environments, where traditional Q-Learning methods fall short. In essence, DQNs utilize deep learning to approximate the Q-function, which estimates the total expected rewards for taking certain actions in given states. This approach enables the handling of vast and intricate input spaces with greater efficiency and effectiveness. By incorporating neural networks, DQNs can generalize across a range of states, learning to navigate and make decisions in intricate environments with an enhanced capacity for interpreting and responding to diverse scenarios as discussed in \cite{ref6}.\hfill \break 

\noindent The core update equation of DQN builds upon the classical Q-Learning update rule, incorporating neural network parameters to enhance learning:

\begin{equation*}
Q^{new}(s, a; \theta) \leftarrow Q(s, a; \theta) + \alpha [R + \gamma \max_{a'} Q(s', a'; \theta^{-}) - Q(s, a; \theta)]
\end{equation*}

In this formula, \(Q(s, a; \theta)\) denotes the neural network's approximation of the action-value function, parameterized by \(\theta\). The term \(R\) represents the  reward, while, \(\gamma\) is the discount factor. The expression \(\max_{a'} Q(s', a'; \theta^{-})\) represents the estimated value of the best possible future action in the new state \(s'\), calculated using the target network parameters \(\theta^{-}\).\hfill \break 

From my DQN implementation, the corresponding code snippet that implements this update rule is found in the \textit{"optimize\_model"} method of the \textit{"Standard\_DQNAgent"} class.

\hfill

\noindent Specifically, the code lines that map to the above equation are: 

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_1.png}}
\caption{Code excerpt showcasing the implementation of the key update rule in the Deep Q-Networks algorithm.}
\label{fig:dqn_code}
\end{figure}

In these lines, \textit{"state\_action\_values"} corresponds to \(Q(s, a; \theta)\), and \textit{"expected\_state\_action\_values"} represents the term \(R + \gamma \max_{a'} Q(s', a'; \theta^{-})\). The loss function \textit{"mse\_loss"} is used to compute the difference between the current and expected Q values, which is then minimized to update the network parameters \(\theta\).\hfill \break  

The following pseudo code outlines the typical steps in a DQN algorithm, aligning closely with the structure and strategy I implemented in my code:

\begin{algorithm}
\caption{Deep Q-Network (DQN) Algorithm}
\begin{algorithmic}[1]
\State Initialize policy network \( Q \) with random weights
\State Initialize target network \( \hat{Q} \) with weights \( \theta \leftarrow \theta^{-} \)
\State Initialize replay buffer \( \mathcal{R} \)
\State Initialize discount factor \( \gamma \)
\State Initialize learning rate \( \alpha \)
\For{each episode}
    \State Initialize state \( s \)
    \While{not done}
        \State Choose action \( a \) from state \( s \) using policy \( Q \) (e.g., epsilon-greedy)
        \State Take action \( a \), observe reward \( r \), next state \( s' \), and done signal
        \State Store transition \( (s, a, r, s') \) in \( \mathcal{R} \)
        \State Sample random minibatch from \( \mathcal{R} \)
        \State Set \( y = r \) if episode terminates at next step, otherwise set \( y = r + \gamma \max_{a'} \hat{Q}(s', a') \)
        \State Perform a gradient descent step on \( (y - Q(s, a))^2 \) with respect to \( Q \)
        \State Every \( C \) steps reset \( \hat{Q} \leftarrow Q \)
        \State \( s \leftarrow s' \)
    \EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}

To summarize, Deep Q-Networks (DQNs) leverage the power of neural networks to approximate action values across large state spaces and uses experience replay for efficient learning, making DQNs a highly effective solution for challenging reinforcement learning tasks.

\subsection{Policy Based Methods}

Policy Based Methods in Reinforcement Learning directly parameterize the policy, which assigns a probability to each action in a given state. Unlike Value Based Methods, which focus on learning a value function that is later used to determine a policy, Policy Based Methods aim to optimize the policy itself without the intermediate step of value estimation. This can be particularly advantageous in high-dimensional or continuous action spaces, where value functions are difficult to represent as can be seen in \cite{ref4}. \hfill \break 

\noindent Policy Based Methods use a specific approach to update the policy, known as the policy gradient method. The equation for this is:

\begin{equation*}
    \nabla_{\theta} J(\theta) = \mathbb{E}_{\pi_{\theta}}[\nabla_{\theta} \log \pi_{\theta}(s, a) \cdot Q^{\pi_{\theta}}(s, a)]
\end{equation*}

In this equation, \(J(\theta)\) represents the objective function, typically the expected return. The term \(\nabla_{\theta} J(\theta)\) is the policy gradient, indicating the direction in which the policy parameters \(\theta\) should be adjusted to maximize \(J(\theta)\). The policy, denoted as \(\pi_{\theta}(s, a)\), gives the probability of selecting action \(a\) in state \(s\), and is parameterized by \(\theta\). \(Q^{\pi_{\theta}}(s, a)\) is the action-value function under the policy \(\pi_{\theta}\), reflecting the expected return for choosing action \(a\) in state \(s\) and following \(\pi_{\theta}\). This function is used to weight the gradient of the logarithm of the policy's probability for the taken action. The expectation \(\mathbb{E}_{\pi_{\theta}}\) signifies that the average is taken over the distribution of states and actions as determined by the policy, thereby guiding the optimization of the policy towards greater expected returns. \hfill \break 

In contrast to Value Based Methods, Policy Based Methods directly manipulate the probability of selecting actions, which allows them to effectively handle situations with stochastic policies and continuous action spaces. The main differences between value-based methods are that policy-based methods do not learn a Q-Value Function and simply learn a policy directly, and policy-based methods can learn stochastic policies unlike value-based methods, which means they may take different actions given the same observation as mentioned in \cite{ref7}. This distinction allows policy-based methods to be more flexible in certain scenarios. Value Based Methods, often have to partition continuous action spaces and may struggle to represent the randomness inherent in some policies. Policy Based Methods often have better convergence properties and can learn stochastic policies, which is a significant advantage over Value Based Methods. However, they typically suffer from high variance in their estimates and often require more samples to achieve comparable results .\hfill \break

An example of a Policy Based Method is the \textit{REINFORCE} algorithm, which utilizes Monte Carlo methods to estimate the policy gradient. By adjusting the policy parameters in the direction suggested by the gradient, the algorithm seeks to maximize the expected cumulative rewards.\hfill \break 

In essence, Policy Based Methods provide a different approach to Reinforcement Learning by optimizing the policy directly. This can lead to more natural learning in environments with complex action spaces and can introduce stochasticity in the policy, which can be beneficial in certain contexts. Despite their differences, both Policy Based and Value Based Methods contribute uniquely to the RL domain, and the choice between them depends on the specific requirements of the problem at hand.

\subsection{Actor-Critic Methods}
Actor-Critic Methods combine the concepts of value-based and policy-based methods. Utilizing two distinct models, the Actor, which suggests actions based on the current state, and the Critic, which evaluates these actions using a value function, Actor-Critic methods effectively leverage the strengths of both policy optimization and value function estimation \cite{ref5}.\hfill \break

\noindent The Actor-Critic architecture can be represented with the following equations:

\begin{equation*}
    \textbf{Actor: } \pi(a|s, \theta) = P[A=a|S=s, \theta]
\end{equation*}
\hfill 
\begin{equation*}
    \textbf{Critic: } V^\pi(s, \omega) = \mathbb{E}[R|S=s, \omega]
\end{equation*}
\break

Here, the Actor, defined by the policy \(\pi(a|s, \theta)\), dictates the likelihood of selecting each possible action \(a\) in a given state \(s\). In contrast, the Critic estimates the value function \(V^\pi(s, \omega)\), reflecting the expected return from state \(s\) under the current policy \(\pi\). The Actor is responsible for action selection, while the Critic assesses the quality of actions taken, guiding the Actor towards better decisions.\hfill \break

\subsubsection{Stochastic Actor-Critic Algorithms}\hfill \break 

Stochastic Actor-Critic Algorithms are a subset of Actor-Critic methods where the policy (Actor) is stochastic in nature. These algorithms model the policy as a probability distribution over actions, enabling the agent to explore a range of actions with varying probabilities. This approach is particularly beneficial in environments where the optimal action is continuous or when exploration is crucial for learning.\hfill \break

In these methods, the Actor outputs probabilities for each action in a given state, and actions are selected based on these probabilities. The Critic evaluates the actions by estimating the value function, providing feedback to the Actor for policy improvement. The stochastic nature of the policy in these methods allows for effective exploration of the action space, making it possible to discover and learn optimal strategies even in complex environments.\hfill \break

One key advantage of Stochastic Actor-Critic Algorithms is their ability to capture multiple potentially good actions in a given state, allowing the agent to learn more robust and generalizable policies. \hfill \break

\noindent \textbf{\textit{PPO-GAE}: An Example of a Stochastic Actor-Critic Algorithm}\hfill \break \break
The Proximal Policy Optimization with Generalized Advantage Estimation (PPO-GAE) algorithm \cite{ref8} is a good example of a Stochastic Actor-Critic method. PPO-GAE optimizes a stochastic policy in a way that carefully balances between exploration and exploitation, using a clipped surrogate objective function and Generalized Advantage Estimation.\hfill \break

\noindent The core update equations for PPO-GAE are:

\begin{equation*}
    L^{CLIP}(\theta) = \mathbb{E}_{t}\left[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)\right]
\end{equation*}

\begin{equation*}
    \hat{A}_t = \delta_t + (\gamma \lambda) \delta_{t+1} + \ldots + (\gamma \lambda)^{T-t+1}\delta_{T-1}
\end{equation*}

\hfill

Here, \(L^{CLIP}(\theta)\) is the objective function optimized during training, incorporating the probability ratio \(r_t(\theta)\) and the advantage estimation \(\hat{A}_t\). The advantage estimation \(\hat{A}_t\) is calculated using Generalized Advantage Estimation (GAE), which combines information across multiple time steps for more efficient learning.\hfill \break

\noindent \textbf{Code Implementation and Explanation from the file \textit{"stochastic\_policy\_approach"}:}\hfill

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_2.png}}
\caption{Code excerpt illustrating the forward pass of the Actor Network in the PPO-GAE algorithm.}
\label{fig:dqn_code}
\end{figure}

In this snippet from the \textit{"ActorNet"} class, the forward pass of the Actor Network is designed to calculate the probabilities for various actions. The network computes the mean (`loc`) and standard deviation (`scale`) for the action distribution, which are essential components in defining the stochastic policy \( \pi(a|s, \theta) \). The `loc` represents the expected value of the actions in a given state, while `scale` denotes the degree of exploration or uncertainty around these actions. This approach allows the Actor to propose actions that are not just optimal but also incorporate an element of randomness, facilitating effective exploration in diverse environmental scenarios.\hfill \break

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_3.png}}
\caption{Code excerpt demonstrating the computation of surrogate losses in the PPO-GAE algorithm.}
\label{fig:dqn_code}
\end{figure}

Here, the surrogate loss for PPO's objective \( L^{CLIP}(\theta) \) is computed. The probability ratio (`rho`) is used along with the advantage estimation (`gae`) to calculate the clipped objective, which aims to limit the policy update's deviation from the previous policy. This snippet can be found in the \textit{"train"} method.\hfill \break

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_4.png}}
\caption{Code excerpt demonstrating the calculation of Generalized Advantage Estimation (GAE) in the PPO-GAE algorithm.}
\label{fig:dqn_code}
\end{figure}

The code for Generalized Advantage Estimation (GAE) calculates the advantage estimation \( \hat{A}_t \) using the temporal difference error (`delta`). This part of the code aligns with the GAE equation and is crucial for efficiently estimating the advantages over multiple time steps. This snippet can be found in the \textit{"RunningMem"} class.\hfill \break

The PPO-GAE algorithm showcases the effective use of stochastic policies in complex environments. The algorithm's balance between exploring diverse strategies and exploiting actions makes it an optimal choice for continuous action spaces.

\subsubsection{Deterministic Actor-Critic Algorithms}\hfill \break 

Deterministic policies, in contrast to stochastic ones, provide a direct mapping from states to actions. This approach is exemplified in algorithms like \textit{Deep Deterministic Policy Gradient (DDPG)} \cite{ref9}. Such methods excel in scenarios with continuous action spaces, offering greater sample efficiency by reducing the variance inherent in action selection. Deterministic policies streamline the learning process, particularly in environments where a clear and consistent response is preferable.\hfill \break

\noindent \textbf{\textit{DDPG}: An Example of a Deterministic Actor-Critic Algorithm}\hfill \break

\noindent The core update equations for DDPG are:

\begin{equation*}
    Q(s, a) = R(s, a) + \gamma Q'(s', \mu'(s'))
\end{equation*}

\begin{equation*}
    \nabla_{\theta^\mu} J \approx \mathbb{E}[\nabla_a Q(s, a|\theta^Q)|_{s=s_t,a=\mu(s_t)} \nabla_{\theta^\mu} \mu(s|\theta^\mu)|_{s_t}]
\end{equation*}

\hfill

Here, \(Q(s, a)\) is the action-value function estimated by the Critic. The Critic is updated using a Bellman-like equation, which includes a reward \(R(s, a)\), a discount factor \(\gamma\), and the action-value function \(Q'\) of the next state-action pair, with the action determined by the target policy \(\mu'\). The Actor (policy function \(\mu(s)\)) is updated using policy gradients, aiming to maximize the expected return as estimated by the Critic. The gradients are calculated with respect to the parameters of the Actor network \(\theta^\mu\) and are based on the gradients of the action-value function \(Q\) provided by the Critic. The algorithm uses target networks \(Q'\) and \(\mu'\) to improve stability.

\hfill

\noindent \textbf{Code Implementation and Explanation from the file \textit{"deterministic\_policy\_approach"}}:\hfill

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_6.png}}
\caption{Code excerpt highlighting the update function in the DDPG algorithm.}
\label{fig:dqn_code}
\end{figure}

This part of the code demonstrates the optimization of both the Actor and Critic networks. The reset of the Ornstein-Uhlenbeck noise process (`ou\_noise.reset()`) is crucial for introducing exploration in the deterministic policy, ensuring that the agent can effectively explore different strategies. The line `Qvals = critic(states, actions)` aligns with the computation of \(Q(s, a)\), while the calculation of `actor\_loss` implements the gradient ascent on the Actor network, reflecting the policy gradient update (\(\nabla_{\theta^\mu} J\)).\hfill \break

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_5.png}}
\caption{Code excerpt demonstrating the forward pass of the Actor Network in the DDPG algorithm.}
\label{fig:dqn_code}
\end{figure}

This code snippet from the \textit{"PolicyNet"} class corresponds to the forward pass of the Actor Network, which is critical for implementing the policy function \(\mu(s)\). The function scales and clips the output to ensure that the actions remain within feasible bounds, directly reflecting the deterministic policy approach in DDPG.\hfill \break

This shows how DDPG is a great example of a deterministic Actor-Critic framework, offering a stable and efficient approach for problems in continuous action spaces. Its structure allows for direct action determination while maintaining the benefits of Actor-Critic methods.\hfill \break

\subsubsection{Conclusion (Policy Based Methods)} \hfill \break

In conclusion, Actor-Critic methods combine policy-based and value-based approaches, offering flexible and effective solutions for various scenarios. Both stochastic methods, like PPO-GAE, and deterministic approaches, such as DDPG, are well-suited for environments with continuous action spaces. Stochastic methods are particularly advantageous for tasks requiring extensive exploration and adaptability, while deterministic methods provide more direct and consistent action mapping, with exploration facilitated through specific techniques like the Ornstein-Uhlenbeck process. The choice between these types depends on the environment's specific demands and the task's goals.

\section{Methodology}

\hfill \break
\textbf{Main Libraries Used:}
Pytorch, Gym, Numpy, Matplotlib, Collections, and OS. 

\subsection{Experiment 1}
\textbf{Problem Addressed:} OpenAI Gym LunarLander (Discrete Actions) - "LunarLander-v2"

The "LunarLander-v2" environment in OpenAI Gym simulates the challenge of safely landing a spacecraft on the lunar surface. The agent controls the spacecraft's thrusters to safely land it between two flags. The state of the environment includes:
\begin{itemize}
    \item Lander's position
    \item Velocity
    \item Angle
    \item Angular velocity
    \item Leg contact with the ground
\end{itemize}
The agent must learn to balance fuel efficiency with the safety and accuracy of the landing.

\noindent The actions in this environment are discrete, comprising of:
\begin{itemize}
    \item Do nothing
    \item Fire the left orientation engine
    \item Fire the main engine
    \item Fire the right orientation engine
\end{itemize}
The reward structure is designed to encourage a safe and fuel-efficient landing. Positive rewards are given for decreasing distance to the landing pad and making a successful landing, while penalties are incurred for moving away from the landing pad, excessive fuel usage, and crashing.

\hfill \break
\noindent \textbf{Validation Measure:} The implementation aimed to develop an agent capable of achieving an average score of 195 or above over the last 50 episodes.

\subsubsection{Implementing Standard DQN} \hfill \break
\vspace{-12pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\ \hline
GAMMA & 0.99 \\ \hline
BATCH\_SIZE & 128 \\ \hline
BUFFER\_SIZE & 10000 \\ \hline
LEARNING\_RATE & 0.0005 \\ \hline
EPS\_START & 1.0 \\ \hline
EPS\_END & 0.01 \\ \hline
EPS\_DECAY & 0.995 \\ \hline
\end{tabular}
\caption{Standard DQN Hyperparameters}
\label{tab:std_dqn_hyperparameters}
\end{table}

\vspace{-12pt}

\noindent \textbf{Model Architecture:} The DQN architecture consists of 3 fully connected layers with ReLU activation functions, designed to approximate the optimal Q-value function:

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_7.png}}
\caption{Code snippet of Standard DQN Architecture}
\label{fig:dqn_code}
\end{figure}

\noindent \textbf{Replay Memory:} The Replay Memory stores and retrieves experiences (state, action, reward, next state, and done) as the agent interacts with the environment. This memory buffer is implemented as a deque (double-ended queue) with a fixed maximum size, enabling efficient sampling of uncorrelated experiences. In this implementation, the \textit{`ReplayMemory`} class initializes a deque with a specified capacity. The `push` method is used to store new experiences, while the `sample` method retrieves a random batch of experiences for training, crucial for effective learning by breaking sequential correlations: 

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_9.png}}
\caption{Code snippet of the Replay Memory}
\label{fig:dqn_code}
\end{figure}

\noindent \textbf{Optimize Model:}This snippet involves sampling from the replay memory, calculating the loss using Mean Squared Error, and updating network weights through backpropagation:

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_8.png}}
\caption{Code snippet of the \textit{Optimize\_model} method}
\label{fig:dqn_code}
\end{figure}

\subsubsection{Implementing Double DQN (First Improvement)} \hfill \break
The implementation of Double DQN aimed to improve the stability and performance of the Standard DQN by addressing the overestimation of Q-values.\newpage

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
Hyperparameter & Value \\ \hline
GAMMA & 0.99 \\ \hline
BATCH\_SIZE & 128 \\ \hline
BUFFER\_SIZE & 10000 \\ \hline
LEARNING\_RATE & 0.0005 \\ \hline
EPS\_START & 1.0 \\ \hline
EPS\_END & 0.01 \\ \hline
EPS\_DECAY & 0.995 \\ \hline
\end{tabular}
\caption{Double DQN Hyperparameters}
\label{tab:double_dqn_hyperparameters}
\end{table}

\vspace{-18pt}

\noindent\textbf{Model Architecture:}
The neural network in Double DQN also consists of three fully connected layers, with ReLU activation functions between layers:

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_10.png}}
\caption{Code snippet of Double DQN Architecture}
\label{fig:dqn_code}
\end{figure}

\noindent\textbf{Action Selection Strategy:}
Double DQN uses an epsilon-greedy approach for action selection, balancing exploration and exploitation:

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_11.png}}
\caption{Code snippet of action selection function}
\label{fig:dqn_code}
\end{figure}

\noindent\textbf{Optimization Process:}
The optimization process in Double DQN differs from Standard DQN in the way it calculates the Q-values for next states. While Standard DQN estimates the future Q-values directly from the target network, Double DQN decouples the selection of actions and the evaluation of those actions. Specifically, it uses the policy network to select the best action for the next state and then uses the target network to evaluate the Q-value of this selected action. This approach helps to mitigate the overestimation bias often observed in Standard DQN, leading to more accurate and stable value estimations:

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_12.png}}
\caption{Code snippet of optimization process function}
\label{fig:dqn_code}
\end{figure}

\noindent\textbf{Updating the Target Network:}
This snippet demonstrates how Double DQN reduces overestimation by using the policy net to choose actions and the target net to evaluate their value. Regularly updating the target network with weights from the policy network is a critical part of the Double DQN algorithm for maintaining stability:

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_13.png}}
\caption{Code snippet of update\_target\_net function}
\label{fig:dqn_code}
\end{figure}

In summary, Double DQN enhances the stability of learning by addressing the overestimation bias present in Standard DQN, making it more reliable for complex environments like "LunarLander-v2."

\subsubsection{Implementing Noisy DQN (Second Improvement)} \hfill

\vspace{-12pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
Hyperparameter & Value \\ \hline
GAMMA & 0.99 \\ \hline
BATCH\_SIZE & 128 \\ \hline
BUFFER\_SIZE & 10000 \\ \hline
LEARNING\_RATE & 0.0005 \\ \hline
EPS\_START & 1.0 \\ \hline
EPS\_END & 0.01 \\ \hline
EPS\_DECAY & 0.995 \\ \hline
\end{tabular}
\caption{Noisy DQN Hyperparameters}
\label{tab:noisy_dqn_hyperparameters}
\end{table}

\vspace{-18pt}

\noindent The Noisy DQN introduces randomness directly into the network's architecture to enhance exploration. This approach eliminates the need for external exploration mechanisms like epsilon-greedy strategies. \hfill \break

\noindent\textbf {NoisyLinear Layer:} The `NoisyLinear` layer is a modified linear layer with noise added to its weights and biases. It uses random noise during training, while reverting to deterministic behaviour  during evaluation:

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_14.png}}
\caption{Code snippet of Noisy Layer}
\label{fig:dqn_code}
\end{figure}

\noindent\textbf{Model Architecture:} The Noisy DQN model utilizes the `NoisyLinear` layers to induce exploration through internal noise. Each layer adds stochasticity to the learning process, which is crucial for exploring the action space effectively:

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_15.png}}
\caption{Code snippet of Noisy DQN Architecture}
\label{fig:dqn_code}
\end{figure}

\noindent\textbf{Optimization Process:}
The optimization process in Noisy DQN resembles standard DQN but leverages the noise-infused network for learning. The loss calculation and backpropagation methods remain consistent with the standard DQN approach. \hfill \break 

These aspects demonstrate the unique strategy of Noisy DQNs in embedding exploration directly within the network's architecture, thus removing the dependence on external exploration strategies. This approach offers a distinct method for addressing challenges in reinforcement learning.

\newpage

\subsubsection{Implementing the combination of the two Improvements (Double \& Noisy) DQN} \hfill

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
Hyperparameter & Value \\ \hline
GAMMA & 0.99 \\ \hline
BATCH\_SIZE & 128 \\ \hline
BUFFER\_SIZE & 10000 \\ \hline
LEARNING\_RATE & 0.0005 \\ \hline
EPS\_START & 1.0 \\ \hline
EPS\_END & 0.01 \\ \hline
EPS\_DECAY & 0.995 \\ \hline
\end{tabular}
\caption{Combined DQN (Double \& Noisy) Hyperparameters}
\label{tab:combined_dqn_hyperparameters}
\end{table}

\vspace{-10pt}

\noindent The Combined DQN model represents a combination of the Double and Noisy DQN approaches. This architecture integrates the \textit{`NoisyLinear`} layers from the Noisy DQN, which introduce stochasticity into the network, enhancing exploration capabilities. Simultaneously, it adopts the \textit{`optimize\_model`} function from the Double DQN, utilizing separate networks for action selection and value evaluation to mitigate overestimation bias in Q-values. Additionally, the combined model leverages the epsilon-greedy strategy for action selection and the efficient experience replay mechanism from both approaches, ensuring a balanced blend of exploration and exploitation. This combination provides a comprehensive solution that leverages both intrinsic exploration through network noise, accurate Q-value estimation, and effective learning from diverse experiences. It should be noted that the code for the \textit{`NoisyLinear`} layers, the \textit{`optimize\_model`} function, and other components like the \textit{'select\_action'} strategy and the \textit{'replay\_memory'} management remains consistent with those outlined in the respective Noisy and Double DQN sections as previously mentioned and explained. \hfill \break

\noindent\textbf{Model Architecture:}
The Combined DQN architecture blends NoisyLinear layers with Double DQN principles:

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_16.png}}
\caption{Code snippet of Combined DQN (Double \& Noisy) Architecture}
\label{fig:dqn_code}
\end{figure}

To summarize, the Combined DQN (Double \& Noisy) model effectively harnesses the benefits of both Double and Noisy DQN strategies. This integration results in a robust approach for the LunarLander-v2 challenge, optimizing both learning efficiency and exploration capabilities.

\subsection{Experiment 2}
\textbf{Problem Addressed:} OpenAI Gym LunarLander (Continuous Actions) - "LunarLanderContinuous-v2"\hfill \break

In the "LunarLanderContinuous-v2" environment, OpenAI Gym presents the task of landing a spacecraft on the lunar surface with continuous action control. This version requires more precise and variable control of the spacecraft's thrusters, adding complexity to the landing maneuver. The agent needs to manage continuous thrust levels for both orientation and descent, aiming for a safe and fuel-efficient landing. The environment's state includes:
\begin{itemize}
    \item Lander's position
    \item Velocity
    \item Angle
    \item Angular velocity
    \item Leg contact with the ground
\end{itemize}
The agent's objective remains to achieve a safe, fuel-efficient landing, but with a continuous range of possible thruster outputs. \hfill \break

\noindent The continuous action space in this environment involves:
\begin{itemize}
    \item Adjusting the main engine's thrust
    \item Controlling the directional thrusters for orientation
\end{itemize}
 \hfill \break
\noindent \textbf{Validation Measure:} Similarly to Experiment 1. The implementation aimed to develop an agent capable of achieving an average score of 195 or above over the last 50 episodes.

\subsubsection{Implementing a Stochastic policy approach (PPO-GAE)} \hfill \break

In the PPO-GAE implementation for "LunarLanderContinuous-v2", the Actor Network, as partially shown in Figure 2, plays a crucial role in proposing actions based on the current state, outputting a probability distribution characterized by mean (\textit{loc}) and standard deviation (\textit{scale}). This network is constructed with fully connected layers followed by \textit{Tanh} activation functions, resulting in the final layer that defines the action distribution. Complementing the Actor, the Critic Network evaluates the proposed actions as can be seen in Figure 17 below. It is responsible for predicting the value function, a measure of the expected return from a given state under the policy in action. The Critic's architecture mirrors the Actor's, consisting of fully connected layers with \textit{Tanh} activations, outputting a single value indicative of the state's value. 

Optimization and Loss Computation in PPO-GAE, as detailed in Figure 3, involve a surrogate objective function to update the Actor network, where the ratio of new and old probabilities of actions is calculated, and a clipping mechanism is employed to moderate policy updates. The Generalized Advantage Estimation (GAE), illustrated in Figure 4, is a key component of the PPO-GAE approach. GAE calculates the advantage estimation, effectively balancing bias and variance in the computation. The Critic Network undergoes updates by minimizing the temporal difference error, which aligns the predicted state values with the computed target values. Training comprises alternating phases of environment interaction for data sampling and network optimization. The Training Loop iteratively performs actions in the environment and stores the resulting experiences. Subsequently, during training epochs, these experiences are processed to compute advantages and update both networks. Reaching or surpassing the validation measure indicates that the agent has successfully solved the environment.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\ \hline
GAMMA & 0.99 \\ \hline
LAMBDA & 0.95 \\ \hline
EPSILON & 0.2 \\ \hline
BATCH\_SIZE & 64 \\ \hline
EPOCH\_REPEAT & 10 \\ \hline
LEARNING\_RATE (Actor) & 0.0003 \\ \hline
LEARNING\_RATE (Critic) & 0.0003 \\ \hline
\end{tabular}
\caption{PPO-GAE Hyperparameters}
\label{tab:ppo_gae_hyperparameters}
\end{table}

\vspace{-20pt}

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.49\textwidth,height=1\textheight,keepaspectratio]{ss_17.png}}
\caption{Code snippet of Critic Network}
\label{fig:dqn_code}
\end{figure}

\subsubsection{Implementing a Deterministic policy approach (DDPG)} \hfill \break

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
Hyperparameter & Value \\ \hline
GAMMA & 0.99 \\ \hline
BATCH\_SIZE & 64 \\ \hline
BUFFER\_SIZE & 10000 \\ \hline
TAU & 0.01 \\ \hline
LEARNING\_RATE (Actor) & 0.0003 \\ \hline
LEARNING\_RATE (Critic) & 0.0003 \\ \hline
\end{tabular}
\caption{DDPG Hyperparameters}
\label{tab:ddpg_hyperparameters}
\end{table}

The update function (Figure 5), a pivotal aspect of DDPG, involves calculating the loss between predicted and target Q-values and updating both the Actor and Critic networks. The Actor is updated using policy gradients to maximize the expected return, as partially shown in Figure 6, which illustrates a snippet of the forward pass through the network. Meanwhile, the Critic is trained to minimize the temporal difference error, ensuring the convergence of the Actor's policy towards optimality. The Replay Memory, integral to the training, stores and samples diverse, uncorrelated experiences, enhancing the learning process. Additionally, the Ornstein-Uhlenbeck process is used for action exploration, introducing time-correlated noise to the actions, aiding in effective exploration of the action space. The training loop involves iteratively applying actions, accumulating rewards, and updating the networks, continuing until the agent achieves a predefined validation measure.

\section{Results and Discussion}
\subsection{Experiment 1}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
Model           & Episodes to Solve \\ \hline
Standard DQN    & 546               \\ \hline
Double DQN      & 462               \\\hline
Noisy DQN       & 422               \\\hline
Combined DQN    & 431               \\ \hline
\end{tabular}
\caption{Number of episodes required for each model to solve the LunarLander-v2 environment.}
\label{tab:ddpg_hyperparameters}
\end{table}

In the ``LunarLander-v2'' environment, the Noisy DQN emerged as the most efficient model, solving the challenge in 422 episodes, which is 22.7\% more quickly than the Standard DQN at 546 episodes. The Double DQN model also displayed notable efficiency, reaching the solution in 462 episodes, approximately 15.4\% faster than the Standard DQN. The Combined DQN (Double \& Noisy DQN) achieved its goal in 431 episodes, standing 21.1\% faster than the Standard DQN. These figures underscore the substantial improvements that noise injection and overestimation bias correction contribute to the efficacy of DQN models, as showcased in Figure 18.

\begin{figure}[h]
\centering
\includegraphics[width=0.49\textwidth,keepaspectratio]{ss_18.png}
\caption{Performance comparison of DQN models in the LunarLander-v2 environment.}
\label{fig:dqn_code}
\end{figure}

\subsection{Experiment 2}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
Model                       & Episodes to Solve \\ \hline
Stochastic Policy (PPO-GAE) & 190               \\ \hline
Deterministic Policy (DDPG) & 449               \\ \hline
\end{tabular}
\caption{Number of episodes required to solve the LunarLanderContinuous-v2 environment.}
\label{tab:ddpg_hyperparameters}
\end{table}

In the LunarLanderContinuous-v2 environment, the Stochastic Policy approach using PPO-GAE solved the task in 190 episodes, demonstrating a significant advantage in efficiency over the Deterministic Policy approach using DDPG, which required 449 episodes to reach the solution. The PPO-GAE approach was more than 57\% faster compared to the DDPG method. This efficiency gain emphasizes the effectiveness of stochastic policy methods in environments with continuous action spaces. Figure 19 provides a visual representation of the learning curves for each method, illustrating the quicker convergence of the PPO-GAE approach to the solution threshold. 

\begin{figure}[h]
\centering
\includegraphics[width=0.49\textwidth,keepaspectratio]{ss_19.png}
\caption{Combined learning curves of the PPO-GAE and DDPG models in the LunarLanderContinuous-v2 environment.}
\label{fig:experiment2_combined_graph}
\end{figure}

\newpage

\printbibliography

\end{document}