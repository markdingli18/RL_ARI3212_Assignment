{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf4743f",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) Agent for LunarLander-v2\n",
    "\n",
    "In this notebook, we'll implement a Deep Q-Network (DQN) to solve the \"LunarLander-v2\" environment from OpenAI Gym. The DQN agent will learn to land a spacecraft safely on the lunar surface. Our goal is to achieve an average score of 195 or above over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad590b",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135bd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e2d9b",
   "metadata": {},
   "source": [
    "### DQN Model\n",
    "\n",
    "Below is the neural network that will approximate the Q-value function. The network will take the state as input and output Q-values for each action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a8c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5762f06",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "\n",
    "We'll use a replay memory to store transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "955d9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b2ba0b",
   "metadata": {},
   "source": [
    "### DQN Agent\n",
    "\n",
    "The DQN agent learns from the environment by interacting with it. It stores experiences and learns by replaying these experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e55008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, batch_size=128, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, learning_rate=0.0005):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.policy_net = DQN(state_size, action_size)\n",
    "        self.target_net = DQN(state_size, action_size)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayMemory(10000)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.epsilon\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.action_size)]], dtype=torch.long)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        done_batch = torch.tensor(batch.done, dtype=torch.float32)\n",
    "\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(self.batch_size)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = (next_state_values * self.gamma) * (1 - done_batch) + reward_batch\n",
    "\n",
    "        loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon_decay * self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a3a4b",
   "metadata": {},
   "source": [
    "### Training the Agent\n",
    "\n",
    "Now, let's train our DQN agent on the LunarLander environment. We will run the environment for a number of episodes and optimize our agent's policy network based on its experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 2/1000 [00:00<01:12, 13.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Average Score: -145.64225312385526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 51/1000 [00:18<05:49,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Average Score: -186.26312916575782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 101/1000 [00:44<09:35,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Average Score: -106.56872105648213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  13%|█▎        | 128/1000 [01:40<1:24:59,  5.85s/it]"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "num_episodes = 1000\n",
    "scores = []\n",
    "\n",
    "# Wrap the episode range with tqdm for the progress bar\n",
    "for i_episode in tqdm(range(num_episodes), desc=\"Training Progress\"):\n",
    "    state_tuple = env.reset()\n",
    "    state = state_tuple[0] if isinstance(state_tuple, tuple) else state_tuple\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        output = env.step(action.item())\n",
    "        next_state = output[0]\n",
    "        reward = output[1]\n",
    "        done = output[2]\n",
    "\n",
    "        next_state = torch.from_numpy(next_state).float().unsqueeze(0)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float)\n",
    "\n",
    "        agent.memory.push(state, action, next_state, reward_tensor, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        agent.optimize_model()\n",
    "\n",
    "    agent.update_target_net()\n",
    "    agent.decay_epsilon()\n",
    "    scores.append(total_reward)\n",
    "\n",
    "    if i_episode % 50 == 0:\n",
    "        print(f\"Episode {i_episode}, Average Score: {np.mean(scores[-50:])}\")\n",
    "\n",
    "    if np.mean(scores[-50:]) >= 195:\n",
    "        print(f\"Solved in {i_episode} episodes!\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
